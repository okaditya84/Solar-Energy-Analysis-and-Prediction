{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469242e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "class SolarEnergyForecasterGB:\n",
    "    def __init__(self,\n",
    "                 plant_file=r'/kaggle/input/solar-power-generation-data/Plant2_filtered.csv',\n",
    "                 weather_file=r'/kaggle/input/solar-power-generation-data/Plant2_Weather_filtered.csv'):\n",
    "        self.plant_file = plant_file\n",
    "        self.weather_file = weather_file\n",
    "        self.results = {}\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load, aggregate and merge plant + weather files. No fourier, no scaling.\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        plant_df = pd.read_csv(self.plant_file)\n",
    "        weather_df = pd.read_csv(self.weather_file)\n",
    "\n",
    "        plant_df['DATE_TIME'] = pd.to_datetime(plant_df['DATE_TIME'])\n",
    "        weather_df['DATE_TIME'] = pd.to_datetime(weather_df['DATE_TIME'])\n",
    "\n",
    "        plant_agg = plant_df.groupby('DATE_TIME').agg({'AC_POWER': 'mean'}).reset_index()\n",
    "        df = pd.merge(weather_df, plant_agg, on='DATE_TIME', how='inner')\n",
    "        df.set_index('DATE_TIME', inplace=True)\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        target_col = 'AC_POWER'\n",
    "        if target_col not in numeric_cols:\n",
    "            raise ValueError(f\"Target column {target_col} not found. Numeric cols: {numeric_cols}\")\n",
    "\n",
    "        feature_cols = [c for c in numeric_cols if c != target_col]\n",
    "        print(f\"Identified {len(feature_cols)} feature columns and target '{target_col}'\")\n",
    "        print(f\"Dataframe shape after merge: {df.shape}\")\n",
    "\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        return df\n",
    "\n",
    "    def create_sequences(self, X, y, time_steps, horizon):\n",
    "        \"\"\"\n",
    "        Create lag-window sequences.\n",
    "        Returns X_seq: (n_samples, time_steps, n_features), y_seq: (n_samples, horizon)\n",
    "        \"\"\"\n",
    "        Xs, ys = [], []\n",
    "        n_total = len(X)\n",
    "        for i in range(n_total - time_steps - horizon + 1):\n",
    "            Xs.append(X[i : i + time_steps])\n",
    "            ys.append(y[i + time_steps : i + time_steps + horizon].flatten())\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    def flatten_sequences_for_trees(self, X_seq):\n",
    "        \"\"\"Flatten 3D sequences to 2D for tree/boosting models.\"\"\"\n",
    "        n_samples, time_steps, n_features = X_seq.shape\n",
    "        return X_seq.reshape(n_samples, time_steps * n_features)\n",
    "\n",
    "    def build_and_train_gb(self, X_train, y_train,\n",
    "                           n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                           min_samples_split=10, subsample=1.0, random_state=RANDOM_STATE):\n",
    "        \"\"\"\n",
    "        Build and fit a classic GradientBoostingRegressor wrapped in MultiOutputRegressor.\n",
    "        Conservative hyperparameters produce a modest model.\n",
    "        \"\"\"\n",
    "        base = GradientBoostingRegressor(\n",
    "            loss='squared_error',\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            subsample=subsample,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        model = MultiOutputRegressor(base)  # multi-output wrapper\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def _per_step_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Per-step MAE, RMSE, R2 and their averages.\"\"\"\n",
    "        horizon = y_true.shape[1]\n",
    "        mae_list, rmse_list, r2_list = [], [], []\n",
    "        for i in range(horizon):\n",
    "            yt = y_true[:, i]\n",
    "            yp = y_pred[:, i]\n",
    "            mae = mean_absolute_error(yt, yp)\n",
    "            rmse = np.sqrt(mean_squared_error(yt, yp))\n",
    "            r2 = r2_score(yt, yp)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            r2_list.append(r2)\n",
    "        return {\n",
    "            'mae_per_step': mae_list,\n",
    "            'rmse_per_step': rmse_list,\n",
    "            'r2_per_step': r2_list,\n",
    "            'mae_avg': float(np.mean(mae_list)),\n",
    "            'rmse_avg': float(np.mean(rmse_list)),\n",
    "            'r2_avg': float(np.mean(r2_list))\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Predict and compute metrics. Returns metrics dict with flattened/per-step/last/avg metrics.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Normalize shapes to 2D arrays\n",
    "        if y_test.ndim == 1:\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "        if y_pred.ndim == 1:\n",
    "            y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "        if y_test.shape[1] != y_pred.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch between y_test {y_test.shape} and y_pred {y_pred.shape}\")\n",
    "\n",
    "        y_test_flat = y_test.flatten()\n",
    "        y_pred_flat = y_pred.flatten()\n",
    "        mae_all = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "        rmse_all = np.sqrt(mean_squared_error(y_test_flat, y_pred_flat))\n",
    "        r2_all = r2_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "        per_step = self._per_step_metrics(y_test, y_pred)\n",
    "        last_idx = y_test.shape[1] - 1\n",
    "        mae_last = per_step['mae_per_step'][last_idx]\n",
    "        rmse_last = per_step['rmse_per_step'][last_idx]\n",
    "        r2_last = per_step['r2_per_step'][last_idx]\n",
    "\n",
    "        metrics = {\n",
    "            'mae_all_flat': mae_all,\n",
    "            'rmse_all_flat': rmse_all,\n",
    "            'r2_all_flat': r2_all,\n",
    "            'mae_last': mae_last,\n",
    "            'rmse_last': rmse_last,\n",
    "            'r2_last': r2_last,\n",
    "            'mae_avg': per_step['mae_avg'],\n",
    "            'rmse_avg': per_step['rmse_avg'],\n",
    "            'r2_avg': per_step['r2_avg'],\n",
    "            'mae_per_step': per_step['mae_per_step'],\n",
    "            'rmse_per_step': per_step['rmse_per_step'],\n",
    "            'r2_per_step': per_step['r2_per_step']\n",
    "        }\n",
    "        return metrics, y_pred\n",
    "\n",
    "    def plot_predictions(self, y_test, y_pred, horizon, metrics, n_plot=200):\n",
    "        \"\"\"Plot actual vs predicted for the first forecast step and print metrics.\"\"\"\n",
    "        if y_test.ndim == 1:\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "        if y_pred.ndim == 1:\n",
    "            y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "        n_samples = min(n_plot, len(y_test))\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:n_samples, 0], label='Actual', alpha=0.8)\n",
    "        plt.plot(y_pred[:n_samples, 0], label='Predicted', alpha=0.8)\n",
    "        plt.title(f'GBR Predictions vs Actual - Horizon {horizon} (First Step)')\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylabel('AC_POWER')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"First-step metrics (Horizon={horizon}) -- MAE: {metrics['mae_per_step'][0]:.2f}, RMSE: {metrics['rmse_per_step'][0]:.2f}, R2: {metrics['r2_per_step'][0]:.4f}\")\n",
    "        print(f\"Last-step metrics (Horizon={horizon})  -- MAE: {metrics['mae_last']:.2f}, RMSE: {metrics['rmse_last']:.2f}, R2: {metrics['r2_last']:.4f}\")\n",
    "        print(f\"Averaged across steps (Horizon={horizon}) -- MAE_avg: {metrics['mae_avg']:.2f}, RMSE_avg: {metrics['rmse_avg']:.2f}, R2_avg: {metrics['r2_avg']:.4f}\")\n",
    "\n",
    "    def run_experiment(self, time_steps=24, horizons=[1,5,24,72], test_frac=0.2, random_state=RANDOM_STATE, summary_agg='avg'):\n",
    "        \"\"\"\n",
    "        Run pipeline and print summary aggregated either by 'last' step or 'avg' across steps.\n",
    "        summary_agg: 'last' or 'avg' (default 'avg')\n",
    "        \"\"\"\n",
    "        if summary_agg not in ('last', 'avg'):\n",
    "            raise ValueError(\"summary_agg must be 'last' or 'avg'\")\n",
    "\n",
    "        df = self.load_and_preprocess_data()\n",
    "\n",
    "        X_all = df[self.feature_cols].values\n",
    "        y_all = df[[self.target_col]].values\n",
    "\n",
    "        print(f\"Raw data shapes: X_all={X_all.shape}, y_all={y_all.shape}\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for horizon in horizons:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"Processing horizon = {horizon}\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            X_seq, y_seq = self.create_sequences(X_all, y_all, time_steps=time_steps, horizon=horizon)\n",
    "            print(f\"Sequence shapes: X_seq={X_seq.shape}, y_seq={y_seq.shape}\")\n",
    "\n",
    "            X_flat = self.flatten_sequences_for_trees(X_seq)\n",
    "            print(f\"Flattened features shape for GBR: {X_flat.shape}\")\n",
    "\n",
    "            split_idx = int(len(X_flat) * (1 - test_frac))\n",
    "            X_train, X_test = X_flat[:split_idx], X_flat[split_idx:]\n",
    "            y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "            print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "            model = self.build_and_train_gb(\n",
    "                X_train, y_train,\n",
    "                n_estimators=200,\n",
    "                max_depth=3,\n",
    "                learning_rate=0.05,\n",
    "                min_samples_split=10,\n",
    "                subsample=1.0,\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            metrics, y_pred = self.evaluate_model(model, X_test, y_test)\n",
    "\n",
    "            model_fname = f'gbr_model_h{horizon}.joblib'\n",
    "            joblib.dump(model, model_fname)\n",
    "            print(f\"Saved model to {model_fname}\")\n",
    "\n",
    "            results[horizon] = {\n",
    "                'model': model,\n",
    "                'metrics': metrics,\n",
    "                'y_pred': y_pred,\n",
    "                'y_test': y_test\n",
    "            }\n",
    "\n",
    "            self.plot_predictions(y_test, y_pred, horizon, metrics, n_plot=200)\n",
    "\n",
    "        self.results = results\n",
    "\n",
    "        # Build summary table including both last-step and averaged metrics\n",
    "        summary_rows = []\n",
    "        for h, r in results.items():\n",
    "            m = r['metrics']\n",
    "            summary_rows.append({\n",
    "                'Horizon': h,\n",
    "                'MAE_last': m['mae_last'],\n",
    "                'RMSE_last': m['rmse_last'],\n",
    "                'R2_last': m['r2_last'],\n",
    "                'MAE_avg': m['mae_avg'],\n",
    "                'RMSE_avg': m['rmse_avg'],\n",
    "                'R2_avg': m['r2_avg']\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_rows).sort_values('Horizon').reset_index(drop=True)\n",
    "\n",
    "        print(\"\\nEXPERIMENT SUMMARY (full table):\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        if summary_agg == 'last':\n",
    "            print(\"\\nSUMMARY (using LAST step metrics):\")\n",
    "            print(summary_df[['Horizon','MAE_last','RMSE_last','R2_last']].to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nSUMMARY (using AVERAGED across steps metrics):\")\n",
    "            print(summary_df[['Horizon','MAE_avg','RMSE_avg','R2_avg']].to_string(index=False))\n",
    "\n",
    "        key = 'RMSE_last' if summary_agg == 'last' else 'RMSE_avg'\n",
    "        best_horizon = summary_df.loc[summary_df[key].idxmin(), 'Horizon']\n",
    "        best_value = summary_df.loc[summary_df[key].idxmin(), key]\n",
    "        print(f\"\\nBest performing horizon by '{key}' (lowest): {best_horizon} with {key}={best_value:.2f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    forecaster = SolarEnergyForecasterGB()\n",
    "\n",
    "    # Change summary_agg to 'last' if you prefer last-step metrics summary.\n",
    "    results = forecaster.run_experiment(\n",
    "        time_steps=24,\n",
    "        horizons=[1, 5, 24, 72],\n",
    "        test_frac=0.2,\n",
    "        summary_agg='avg'   # 'avg' or 'last'\n",
    "    )\n",
    "\n",
    "    print(\"\\nExperiment completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
