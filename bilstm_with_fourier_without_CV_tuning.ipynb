{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Bidirectional, LSTM, Dense, Dropout, \n",
    "                                   MultiHeadAttention, LayerNormalization, \n",
    "                                   Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "                                   BatchNormalization, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Enhanced BiLSTM Solar Power Prediction Model\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Loading and Preprocessing\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess solar generation and weather data with enhanced features\"\"\"\n",
    "    \n",
    "    # Load generation data\n",
    "    gen_data = pd.read_csv('Datasets/Processed Datasets/Plant1_filtered.csv')\n",
    "    gen_data['DATE_TIME'] = pd.to_datetime(gen_data['DATE_TIME'], format='mixed')\n",
    "    \n",
    "    # Aggregate AC_POWER across all inverters for each timestamp\n",
    "    gen_agg = gen_data.groupby('DATE_TIME').agg({\n",
    "        'AC_POWER': 'sum',\n",
    "        'DC_POWER': 'sum',\n",
    "        'DAILY_YIELD': 'mean'\n",
    "    }).reset_index()\n",
    "    gen_agg.columns = ['DATE_TIME', 'total_AC_POWER', 'total_DC_POWER', 'avg_DAILY_YIELD']\n",
    "    \n",
    "    # Load weather data\n",
    "    weather_data = pd.read_csv('Datasets/Processed Datasets/Plant1_Weather_filtered.csv')\n",
    "    weather_data['DATE_TIME'] = pd.to_datetime(weather_data['DATE_TIME'])\n",
    "    \n",
    "    # Merge datasets\n",
    "    data = pd.merge(gen_agg, weather_data[['DATE_TIME', 'AMBIENT_TEMPERATURE', \n",
    "                                          'MODULE_TEMPERATURE', 'IRRADIATION']], \n",
    "                   on='DATE_TIME', how='inner')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    data = data.sort_values('DATE_TIME').reset_index(drop=True)\n",
    "    \n",
    "    # Data cleaning and outlier removal\n",
    "    print(f\"Initial data shape: {data.shape}\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Remove negative values\n",
    "    data = data[(data['total_AC_POWER'] >= 0) & (data['IRRADIATION'] >= 0)]\n",
    "    \n",
    "    # Remove outliers using IQR method for AC_POWER\n",
    "    Q1 = data['total_AC_POWER'].quantile(0.25)\n",
    "    Q3 = data['total_AC_POWER'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data = data[(data['total_AC_POWER'] >= lower_bound) & \n",
    "                (data['total_AC_POWER'] <= upper_bound)]\n",
    "    \n",
    "    print(f\"Data shape after cleaning: {data.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_enhanced_features(data):\n",
    "    \"\"\"Create enhanced temporal and derived features\"\"\"\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    # 1. Basic time features\n",
    "    data['hour'] = data['DATE_TIME'].dt.hour\n",
    "    data['day_of_year'] = data['DATE_TIME'].dt.dayofyear\n",
    "    data['month'] = data['DATE_TIME'].dt.month\n",
    "    data['day_of_week'] = data['DATE_TIME'].dt.dayofweek\n",
    "    data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # 2. Cyclical encoding for temporal features\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    # 3. Solar position features (simplified)\n",
    "    data['solar_elevation'] = np.sin(2 * np.pi * (data['hour'] - 6) / 12) * \\\n",
    "                             np.sin(2 * np.pi * data['day_of_year'] / 365)\n",
    "    data['solar_elevation'] = np.maximum(0, data['solar_elevation'])\n",
    "    \n",
    "    # 4. Fourier features for multiple periodicities\n",
    "    data['time_index'] = range(len(data))\n",
    "    \n",
    "    # Daily periodicity (96 intervals per day for 15-min data)\n",
    "    for i in range(1, 8):  # 7 harmonics for daily cycle\n",
    "        data[f'daily_sin_{i}'] = np.sin(2 * np.pi * i * data['time_index'] / 96)\n",
    "        data[f'daily_cos_{i}'] = np.cos(2 * np.pi * i * data['time_index'] / 96)\n",
    "    \n",
    "    # Weekly periodicity (672 intervals per week)\n",
    "    for i in range(1, 4):  # 3 harmonics for weekly cycle\n",
    "        data[f'weekly_sin_{i}'] = np.sin(2 * np.pi * i * data['time_index'] / 672)\n",
    "        data[f'weekly_cos_{i}'] = np.cos(2 * np.pi * i * data['time_index'] / 672)\n",
    "    \n",
    "    # 5. Derived physics-based features\n",
    "    data['efficiency'] = data['total_AC_POWER'] / (data['total_DC_POWER'] + 1e-8)\n",
    "    data['temp_diff'] = data['MODULE_TEMPERATURE'] - data['AMBIENT_TEMPERATURE']\n",
    "    data['irr_temp_ratio'] = data['IRRADIATION'] / (data['MODULE_TEMPERATURE'] + 1e-8)\n",
    "    data['power_per_irr'] = data['total_AC_POWER'] / (data['IRRADIATION'] + 1e-8)\n",
    "    \n",
    "    # 6. Lag features for temporal dependencies\n",
    "    lag_cols = ['total_AC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']\n",
    "    lag_periods = [1, 2, 4, 8, 16, 24, 48, 96]  # 15min, 30min, 1h, 2h, 4h, 6h, 12h, 24h\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        for lag in lag_periods:\n",
    "            data[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
    "    \n",
    "    # 7. Rolling statistics\n",
    "    windows = [4, 8, 16, 24]  # 1h, 2h, 4h, 6h windows\n",
    "    for col in lag_cols:\n",
    "        for window in windows:\n",
    "            data[f'{col}_rolling_mean_{window}'] = data[col].rolling(window=window, min_periods=1).mean()\n",
    "            data[f'{col}_rolling_std_{window}'] = data[col].rolling(window=window, min_periods=1).std()\n",
    "            data[f'{col}_rolling_max_{window}'] = data[col].rolling(window=window, min_periods=1).max()\n",
    "            data[f'{col}_rolling_min_{window}'] = data[col].rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    # 8. Interaction features\n",
    "    data['irr_amb_interaction'] = data['IRRADIATION'] * data['AMBIENT_TEMPERATURE']\n",
    "    data['irr_mod_interaction'] = data['IRRADIATION'] * data['MODULE_TEMPERATURE']\n",
    "    data['temp_interaction'] = data['AMBIENT_TEMPERATURE'] * data['MODULE_TEMPERATURE']\n",
    "    \n",
    "    # Remove rows with NaN values from feature engineering\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    data = data.drop(['time_index'], axis=1)\n",
    "    \n",
    "    print(f\"Data shape after feature engineering: {data.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "data = load_and_preprocess_data()\n",
    "data = create_enhanced_features(data)\n",
    "\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "print(f\"Date range: {data['DATE_TIME'].min()} to {data['DATE_TIME'].max()}\")\n",
    "print(f\"AC_POWER range: {data['total_AC_POWER'].min():.2f} to {data['total_AC_POWER'].max():.2f} kW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Feature Selection\n",
    "def select_features(data):\n",
    "    \"\"\"Select the most relevant features for the model - reduced set to prevent overfitting\"\"\"\n",
    "    \n",
    "    # Core essential features\n",
    "    base_features = ['total_DC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']\n",
    "    \n",
    "    # Key temporal features (reduced set)\n",
    "    temporal_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'solar_elevation']\n",
    "    \n",
    "    # Select only the most important Fourier features\n",
    "    fourier_features = [f'daily_sin_{i}' for i in range(1, 4)] + [f'daily_cos_{i}' for i in range(1, 4)]\n",
    "    \n",
    "    # Key derived features\n",
    "    derived_features = ['efficiency', 'temp_diff', 'irr_temp_ratio']\n",
    "    \n",
    "    # Select only short-term lag features (most relevant)\n",
    "    lag_features = [col for col in data.columns if '_lag_' in col and \n",
    "                   any(f'_lag_{lag}' in col for lag in [1, 2, 4, 8])]\n",
    "    \n",
    "    # Select only key rolling features (reduce window sizes)\n",
    "    rolling_features = [col for col in data.columns if '_rolling_mean_' in col and\n",
    "                       any(f'_{window}' in col for window in [4, 8])]\n",
    "    \n",
    "    # Combine selected features (much smaller set)\n",
    "    selected_features = (base_features + temporal_features + fourier_features + \n",
    "                        derived_features + lag_features[:16] + rolling_features[:8])  # Limit total features\n",
    "    \n",
    "    # Filter features that exist in data\n",
    "    selected_features = [f for f in selected_features if f in data.columns]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for modeling (reduced from original)\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "def create_sequences_multi_horizon(data, features, target, seq_length, horizons):\n",
    "    \"\"\"Create sequences for multi-horizon forecasting\"\"\"\n",
    "    \n",
    "    X, y_dict = [], {f'horizon_{h}': [] for h in horizons}\n",
    "    \n",
    "    max_horizon = max(horizons)\n",
    "    \n",
    "    for i in range(len(data) - seq_length - max_horizon + 1):\n",
    "        # Input sequence\n",
    "        X.append(data[features].iloc[i:i + seq_length].values)\n",
    "        \n",
    "        # Multi-horizon targets\n",
    "        for h in horizons:\n",
    "            target_idx = i + seq_length + h - 1\n",
    "            y_dict[f'horizon_{h}'].append(data[target].iloc[target_idx])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y_arrays = {key: np.array(values) for key, values in y_dict.items()}\n",
    "    \n",
    "    return X, y_arrays\n",
    "\n",
    "# Build Improved BiLSTM Model\n",
    "def build_enhanced_bilstm_model(input_shape, horizons):\n",
    "    \"\"\"Build an improved BiLSTM model with better regularization and architecture\"\"\"\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='input_sequence')\n",
    "    \n",
    "    # Lighter feature extraction with proper normalization\n",
    "    x = Dense(64, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Simplified BiLSTM architecture with better regularization\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, \n",
    "                          recurrent_dropout=0.2, kernel_regularizer=tf.keras.regularizers.l2(0.001)))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Second BiLSTM layer (smaller to prevent overfitting)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=False, dropout=0.3,\n",
    "                          recurrent_dropout=0.2, kernel_regularizer=tf.keras.regularizers.l2(0.001)))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Simplified dense layers\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Multi-horizon outputs with shared representation\n",
    "    outputs = {}\n",
    "    for h in horizons:\n",
    "        # Simpler output head\n",
    "        output = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), \n",
    "                      name=f'dense_horizon_{h}')(x)\n",
    "        output = Dropout(0.2)(output)\n",
    "        outputs[f'horizon_{h}'] = Dense(1, name=f'output_horizon_{h}')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "    \n",
    "    # More balanced loss weights and lower learning rate\n",
    "    loss_weights = {f'output_horizon_{h}': 1.0 / (h ** 0.3) for h in horizons}\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005, clipnorm=0.5),  # Lower learning rate and gradient clipping\n",
    "        loss={f'output_horizon_{h}': 'huber' for h in horizons},  # Huber loss is more robust\n",
    "        loss_weights=loss_weights,\n",
    "        metrics={f'output_horizon_{h}': ['mae', 'mse'] for h in horizons}\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare features and target\n",
    "print(\"\\nPreparing features...\")\n",
    "selected_features = select_features(data)\n",
    "target_col = 'total_AC_POWER'\n",
    "\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"- Base features: {len([f for f in selected_features if any(base in f for base in ['DC_POWER', 'AMBIENT', 'MODULE', 'IRRADIATION'])])}\")\n",
    "print(f\"- Temporal features: {len([f for f in selected_features if any(temp in f for temp in ['_sin', '_cos', 'solar', 'weekend'])])}\")\n",
    "print(f\"- Lag features: {len([f for f in selected_features if '_lag_' in f])}\")\n",
    "print(f\"- Rolling features: {len([f for f in selected_features if '_rolling_' in f])}\")\n",
    "print(f\"- Derived features: {len([f for f in selected_features if any(der in f for der in ['efficiency', 'temp_diff', 'ratio', 'interaction'])])}\")\n",
    "\n",
    "# Define prediction horizons (in time steps: 15-min intervals)\n",
    "prediction_horizons = [4, 20, 96, 288]  # 1 hour, 5 hours, 1 day, 3 days\n",
    "horizon_names = ['1_hour', '5_hours', '1_day', '3_days']\n",
    "\n",
    "print(f\"\\nPrediction horizons: {dict(zip(horizon_names, prediction_horizons))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning and Cross-Validation Setup\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import itertools\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import gc\n",
    "\n",
    "# Custom TimeSeriesCV class for multi-horizon predictions\n",
    "class TimeSeriesCV:\n",
    "    \"\"\"Custom time series cross-validation for multi-horizon forecasting\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, test_size=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "    \n",
    "    def split(self, X, y=None):\n",
    "        n_samples = len(X)\n",
    "        if self.test_size is None:\n",
    "            test_size = n_samples // (self.n_splits + 1)\n",
    "        else:\n",
    "            test_size = self.test_size\n",
    "        \n",
    "        splits = []\n",
    "        for i in range(self.n_splits):\n",
    "            # Calculate split indices\n",
    "            test_end = n_samples - i * (test_size // 2)\n",
    "            test_start = test_end - test_size\n",
    "            train_end = test_start\n",
    "            train_start = 0\n",
    "            \n",
    "            if train_end <= train_start or test_start <= 0:\n",
    "                break\n",
    "                \n",
    "            train_idx = list(range(train_start, train_end))\n",
    "            test_idx = list(range(test_start, test_end))\n",
    "            \n",
    "            splits.append((train_idx, test_idx))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# Custom Multi-Output BiLSTM Wrapper for scikit-learn compatibility\n",
    "class MultiHorizonBiLSTM(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wrapper class for BiLSTM to work with scikit-learn CV\"\"\"\n",
    "    \n",
    "    def __init__(self, lstm_units_1=64, lstm_units_2=32, dense_units=32, dropout_rate=0.3, \n",
    "                 recurrent_dropout=0.2, learning_rate=0.0005, l2_reg=0.001, batch_size=64):\n",
    "        self.lstm_units_1 = lstm_units_1\n",
    "        self.lstm_units_2 = lstm_units_2\n",
    "        self.dense_units = dense_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_reg = l2_reg\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.input_shape = None\n",
    "        self.horizons = None\n",
    "        \n",
    "    def build_model(self, input_shape, horizons):\n",
    "        \"\"\"Build the BiLSTM model with current hyperparameters\"\"\"\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        inputs = Input(shape=input_shape, name='input_sequence')\n",
    "        \n",
    "        # Feature extraction\n",
    "        x = Dense(self.dense_units, activation='relu', \n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(self.dropout_rate * 0.7)(x)\n",
    "        \n",
    "        # First BiLSTM layer\n",
    "        x = Bidirectional(LSTM(self.lstm_units_1, return_sequences=True, \n",
    "                              dropout=self.dropout_rate, \n",
    "                              recurrent_dropout=self.recurrent_dropout,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)))(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        # Second BiLSTM layer\n",
    "        x = Bidirectional(LSTM(self.lstm_units_2, return_sequences=False, \n",
    "                              dropout=self.dropout_rate,\n",
    "                              recurrent_dropout=self.recurrent_dropout,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)))(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(self.dropout_rate * 1.2)(x)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = Dense(self.dense_units, activation='relu', \n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        # Multi-horizon outputs\n",
    "        outputs = {}\n",
    "        for h in horizons:\n",
    "            output = Dense(16, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg), \n",
    "                          name=f'dense_horizon_{h}')(x)\n",
    "            output = Dropout(self.dropout_rate * 0.7)(output)\n",
    "            outputs[f'horizon_{h}'] = Dense(1, name=f'output_horizon_{h}')(output)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "        \n",
    "        # Compile model\n",
    "        loss_weights = {f'output_horizon_{h}': 1.0 / (h ** 0.3) for h in horizons}\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate, clipnorm=0.5),\n",
    "            loss={f'output_horizon_{h}': 'huber' for h in horizons},\n",
    "            loss_weights=loss_weights,\n",
    "            metrics={f'output_horizon_{h}': ['mae'] for h in horizons}\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the model\"\"\"\n",
    "        self.input_shape = (X.shape[1], X.shape[2])\n",
    "        \n",
    "        # Determine horizons from y structure\n",
    "        if isinstance(y, dict):\n",
    "            self.horizons = [int(key.split('_')[-1]) for key in y.keys() if 'horizon' in key]\n",
    "            y_list = [y[f'output_horizon_{h}'] for h in self.horizons]\n",
    "        else:\n",
    "            # Assume y is a list of arrays for different horizons\n",
    "            self.horizons = prediction_horizons\n",
    "            y_list = y if isinstance(y, list) else [y]\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(self.input_shape, self.horizons)\n",
    "        \n",
    "        # Prepare y as dictionary for multi-output\n",
    "        y_dict = {f'output_horizon_{h}': y_arr for h, y_arr in zip(self.horizons, y_list)}\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X, y_dict,\n",
    "            epochs=30,  # Reduced for CV\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=0.15,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True, min_delta=0.001)]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "        \n",
    "        predictions = self.model.predict(X, verbose=0)\n",
    "        \n",
    "        # Return predictions as a single array (average across horizons for CV scoring)\n",
    "        if isinstance(predictions, list):\n",
    "            return np.mean([pred.flatten() for pred in predictions], axis=0)\n",
    "        else:\n",
    "            return predictions.flatten()\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R² score (for compatibility with CV)\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Handle multi-output y\n",
    "        if isinstance(y, dict):\n",
    "            # Use first horizon for scoring in CV\n",
    "            first_horizon = min(self.horizons)\n",
    "            y_true = y[f'output_horizon_{first_horizon}']\n",
    "        elif isinstance(y, list):\n",
    "            y_true = y[0]  # First horizon\n",
    "        else:\n",
    "            y_true = y\n",
    "        \n",
    "        return r2_score(y_true, y_pred[:len(y_true)])\n",
    "\n",
    "print(\"Hyperparameter tuning setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Data Preparation and Scaling\n",
    "print(\"\\nPreparing data for training...\")\n",
    "\n",
    "# Separate features and target\n",
    "feature_data = data[selected_features].copy()\n",
    "target_data = data[target_col].copy()\n",
    "\n",
    "# Improved target transformation - use Box-Cox or simpler sqrt transformation\n",
    "from scipy import stats\n",
    "target_data_transformed = np.sqrt(target_data + 1)  # Square root transformation for better distribution\n",
    "\n",
    "# Use MinMaxScaler for features (often works better for LSTM) and RobustScaler for target\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "feature_scaler = MinMaxScaler(feature_range=(-1, 1))  # Symmetric range\n",
    "target_scaler = RobustScaler()  # More robust to outliers\n",
    "\n",
    "# Fit scalers on training data only (chronological split)\n",
    "train_size = int(0.8 * len(feature_data))\n",
    "\n",
    "# Scale features\n",
    "train_features = feature_data[:train_size]\n",
    "test_features = feature_data[train_size:]\n",
    "feature_data_scaled = np.zeros_like(feature_data)\n",
    "feature_data_scaled[:train_size] = feature_scaler.fit_transform(train_features)\n",
    "feature_data_scaled[train_size:] = feature_scaler.transform(test_features)\n",
    "\n",
    "# Scale target\n",
    "train_target = target_data_transformed[:train_size]\n",
    "test_target = target_data_transformed[train_size:]\n",
    "target_data_scaled = np.zeros_like(target_data_transformed)\n",
    "target_data_scaled[:train_size] = target_scaler.fit_transform(train_target.values.reshape(-1, 1)).flatten()\n",
    "target_data_scaled[train_size:] = target_scaler.transform(test_target.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create DataFrame with scaled data\n",
    "scaled_df = pd.DataFrame(feature_data_scaled, columns=selected_features)\n",
    "scaled_df['target'] = target_data_scaled\n",
    "scaled_df['DATE_TIME'] = data['DATE_TIME'].values\n",
    "\n",
    "print(f\"Training on first {train_size} samples ({train_size/len(data)*100:.1f}%)\")\n",
    "print(f\"Testing on last {len(data)-train_size} samples ({(len(data)-train_size)/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Reduced sequence length to prevent overfitting\n",
    "seq_length = 24  # 6 hours of history (24 * 15 minutes) - reduced from 48\n",
    "print(f\"Using sequence length: {seq_length} time steps (6 hours)\")\n",
    "\n",
    "X, y_horizons = create_sequences_multi_horizon(\n",
    "    scaled_df, selected_features, 'target', seq_length, prediction_horizons\n",
    ")\n",
    "\n",
    "print(f\"Created sequences: X shape = {X.shape}\")\n",
    "for h, horizon in zip(horizon_names, prediction_horizons):\n",
    "    print(f\"Y shape for {h}: {y_horizons[f'horizon_{horizon}'].shape}\")\n",
    "\n",
    "# Split data chronologically\n",
    "train_samples = train_size - seq_length - max(prediction_horizons)\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "\n",
    "y_train = {f'output_horizon_{h}': y_horizons[f'horizon_{h}'][:train_samples] \n",
    "           for h in prediction_horizons}\n",
    "y_test = {f'output_horizon_{h}': y_horizons[f'horizon_{h}'][train_samples:] \n",
    "          for h in prediction_horizons}\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Data validation\n",
    "print(f\"\\nData validation:\")\n",
    "print(f\"Feature data range: [{feature_data_scaled.min():.3f}, {feature_data_scaled.max():.3f}]\")\n",
    "print(f\"Target data range: [{target_data_scaled.min():.3f}, {target_data_scaled.max():.3f}]\")\n",
    "print(f\"Target std: {target_data_scaled.std():.3f}\")\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "print(\"\\nTraining final model with CV-tuned hyperparameters...\")\n",
    "final_model, history = train_best_model(best_params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1880c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search and Cross-Validation\n",
    "def perform_hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Perform hyperparameter tuning with time series cross-validation\"\"\"\n",
    "    \n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'lstm_units_1': [32, 64, 96],\n",
    "        'lstm_units_2': [16, 32, 48],\n",
    "        'dense_units': [16, 32, 48],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4],\n",
    "        'learning_rate': [0.0003, 0.0005, 0.001],\n",
    "        'l2_reg': [0.0005, 0.001, 0.002],\n",
    "        'batch_size': [32, 64]\n",
    "    }\n",
    "    \n",
    "    print(f\"Parameter grid size: {np.prod([len(v) for v in param_grid.values()])} combinations\")\n",
    "    \n",
    "    # Randomized search for efficiency (testing subset of combinations)\n",
    "    n_iter = 20  # Number of random combinations to test\n",
    "    print(f\"Using RandomizedSearchCV with {n_iter} iterations\")\n",
    "    \n",
    "    # Create time series CV object\n",
    "    tscv = TimeSeriesCV(n_splits=3, test_size=len(X_train)//5)\n",
    "    \n",
    "    # Create model wrapper\n",
    "    model_wrapper = MultiHorizonBiLSTM()\n",
    "    \n",
    "    # Perform randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model_wrapper,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        cv=tscv,\n",
    "        scoring='r2',\n",
    "        n_jobs=1,  # Sequential due to memory constraints\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the search\n",
    "    print(\"Fitting hyperparameter search...\")\n",
    "    search_results = random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Hyperparameter tuning completed!\")\n",
    "    print(f\"Best score: {search_results.best_score_:.4f}\")\n",
    "    print(f\"Best parameters: {search_results.best_params_}\")\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "def evaluate_cv_results(search_results):\n",
    "    \"\"\"Analyze and visualize CV results\"\"\"\n",
    "    \n",
    "    # Extract results\n",
    "    results_df = pd.DataFrame(search_results.cv_results_)\n",
    "    \n",
    "    print(\"\\nTop 5 parameter combinations:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Sort by mean test score\n",
    "    top_results = results_df.nlargest(5, 'mean_test_score')\n",
    "    \n",
    "    for i, (idx, row) in enumerate(top_results.iterrows()):\n",
    "        print(f\"\\n{i+1}. Score: {row['mean_test_score']:.4f} (±{row['std_test_score']:.4f})\")\n",
    "        params = row['params']\n",
    "        for param, value in params.items():\n",
    "            print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Analyze parameter importance\n",
    "    print(\"\\nParameter Analysis:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    important_params = ['lstm_units_1', 'lstm_units_2', 'dropout_rate', 'learning_rate']\n",
    "    \n",
    "    for param in important_params:\n",
    "        param_values = [result['params'][param] for result in search_results.cv_results_['params']]\n",
    "        param_scores = search_results.cv_results_['mean_test_score']\n",
    "        \n",
    "        # Group by parameter value and calculate mean score\n",
    "        param_score_dict = {}\n",
    "        for val, score in zip(param_values, param_scores):\n",
    "            if val not in param_score_dict:\n",
    "                param_score_dict[val] = []\n",
    "            param_score_dict[val].append(score)\n",
    "        \n",
    "        print(f\"\\n{param}:\")\n",
    "        for val, scores in param_score_dict.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"  {val}: {mean_score:.4f} (±{np.std(scores):.4f})\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def train_best_model(best_params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train the final model with best hyperparameters\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining final model with best parameters...\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    best_model = MultiHorizonBiLSTM(**best_params)\n",
    "    \n",
    "    # Build the actual model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    final_model = best_model.build_model(input_shape, prediction_horizons)\n",
    "    \n",
    "    # Enhanced callbacks for final training\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, \n",
    "                     verbose=1, min_delta=0.0001),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=7, \n",
    "                         min_lr=1e-7, verbose=1, min_delta=0.0001),\n",
    "        ModelCheckpoint('best_cv_tuned_bilstm_model.h5', save_best_only=True, \n",
    "                       monitor='val_loss', verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # Prepare validation data\n",
    "    if isinstance(y_val, dict):\n",
    "        val_data = (X_val, y_val)\n",
    "    else:\n",
    "        val_data = (X_val, {f'output_horizon_{h}': y_val[i] \n",
    "                           for i, h in enumerate(prediction_horizons)})\n",
    "    \n",
    "    # Train final model\n",
    "    print(\"Training final model...\")\n",
    "    history = final_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=val_data,\n",
    "        epochs=100,\n",
    "        batch_size=best_params.get('batch_size', 64),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return final_model, history\n",
    "\n",
    "# Prepare data for CV and hyperparameter tuning\n",
    "print(\"Preparing data for hyperparameter tuning...\")\n",
    "\n",
    "# Use a smaller validation set for hyperparameter tuning\n",
    "val_size = len(X_train) // 5\n",
    "X_hp_train = X_train[:-val_size]\n",
    "X_hp_val = X_train[-val_size:]\n",
    "\n",
    "y_hp_train = {key: val[:-val_size] for key, val in y_train.items()}\n",
    "y_hp_val = {key: val[-val_size:] for key, val in y_train.items()}\n",
    "\n",
    "print(f\"HP tuning - Train: {X_hp_train.shape[0]}, Val: {X_hp_val.shape[0]} samples\")\n",
    "\n",
    "# Check if we should perform hyperparameter tuning\n",
    "perform_tuning = True  # Set to False to skip tuning and use default parameters\n",
    "\n",
    "if perform_tuning:\n",
    "    # Perform hyperparameter search\n",
    "    search_results = perform_hyperparameter_tuning(X_hp_train, y_hp_train, X_hp_val, y_hp_val)\n",
    "    \n",
    "    # Analyze results\n",
    "    results_df = evaluate_cv_results(search_results)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = search_results.best_params_\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters found:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "else:\n",
    "    # Use manually optimized parameters (skip tuning for faster execution)\n",
    "    print(\"Using manually optimized parameters (skipping automated tuning)...\")\n",
    "    best_params = {\n",
    "        'lstm_units_1': 64,\n",
    "        'lstm_units_2': 32,\n",
    "        'dense_units': 32,\n",
    "        'dropout_rate': 0.3,\n",
    "        'learning_rate': 0.0005,\n",
    "        'l2_reg': 0.001,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "    print(f\"Parameters: {best_params}\")\n",
    "\n",
    "print(\"\\nHyperparameter selection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f27771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test, target_scaler, prediction_horizons, horizon_names):\n",
    "    \"\"\"Evaluate model performance across all prediction horizons with improved scaling\"\"\"\n",
    "    \n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, (h, h_name) in enumerate(zip(prediction_horizons, horizon_names)):\n",
    "        # Get predictions for this horizon\n",
    "        y_pred_scaled = predictions[i].flatten()\n",
    "        y_true_scaled = y_test[f'output_horizon_{h}']\n",
    "        \n",
    "        # Inverse transform predictions (from scaled back to sqrt-transformed)\n",
    "        y_pred_sqrt = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        y_true_sqrt = target_scaler.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Convert from sqrt scale to original scale\n",
    "        y_pred_orig = np.maximum(0, y_pred_sqrt ** 2 - 1)  # Inverse of sqrt(x + 1)\n",
    "        y_true_orig = np.maximum(0, y_true_sqrt ** 2 - 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "        \n",
    "        # Improved MAPE calculation\n",
    "        mask = y_true_orig > 1  # Only calculate MAPE where true values > 1 to avoid division issues\n",
    "        if mask.sum() > 0:\n",
    "            mape = np.mean(np.abs((y_true_orig[mask] - y_pred_orig[mask]) / y_true_orig[mask])) * 100\n",
    "        else:\n",
    "            mape = float('inf')\n",
    "        \n",
    "        r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "        \n",
    "        # Additional metrics\n",
    "        mean_true = np.mean(y_true_orig)\n",
    "        mean_pred = np.mean(y_pred_orig)\n",
    "        std_true = np.std(y_true_orig)\n",
    "        std_pred = np.std(y_pred_orig)\n",
    "        \n",
    "        results[h_name] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'r2': r2,\n",
    "            'mean_true': mean_true,\n",
    "            'mean_pred': mean_pred,\n",
    "            'std_true': std_true,\n",
    "            'std_pred': std_pred,\n",
    "            'y_true': y_true_orig,\n",
    "            'y_pred': y_pred_orig\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{h_name.replace('_', ' ').title()} Prediction:\")\n",
    "        print(f\"  MAE:  {mae:.2f} kW\")\n",
    "        print(f\"  RMSE: {rmse:.2f} kW\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\" if mape != float('inf') else \"  MAPE: N/A (low values)\")\n",
    "        print(f\"  R²:   {r2:.4f}\")\n",
    "        print(f\"  Mean True: {mean_true:.2f} kW, Mean Pred: {mean_pred:.2f} kW\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_prediction_function(model, feature_scaler, target_scaler, selected_features, seq_length):\n",
    "    \"\"\"Create a function for making predictions with current conditions\"\"\"\n",
    "    \n",
    "    def predict_future_power(current_ac_power, ambient_temp, module_temp, irradiation, \n",
    "                           current_time=None):\n",
    "        \"\"\"\n",
    "        Predict future AC power given current conditions\n",
    "        \n",
    "        Parameters:\n",
    "        - current_ac_power: Current AC power (kW)\n",
    "        - ambient_temp: Current ambient temperature (°C)\n",
    "        - module_temp: Current module temperature (°C)\n",
    "        - irradiation: Current irradiation (kW/m²)\n",
    "        - current_time: Current datetime (if None, uses current time)\n",
    "        \"\"\"\n",
    "        \n",
    "        if current_time is None:\n",
    "            current_time = datetime.now()\n",
    "        \n",
    "        # Create a dummy sequence with repeated current conditions\n",
    "        # In practice, you would use actual historical data\n",
    "        dummy_data = pd.DataFrame({\n",
    "            'total_DC_POWER': [current_ac_power * 1.1] * seq_length,  # Assume 10% DC/AC loss\n",
    "            'AMBIENT_TEMPERATURE': [ambient_temp] * seq_length,\n",
    "            'MODULE_TEMPERATURE': [module_temp] * seq_length,\n",
    "            'IRRADIATION': [irradiation] * seq_length,\n",
    "        })\n",
    "        \n",
    "        # Generate time-based features for the sequence\n",
    "        time_seq = pd.date_range(end=current_time, periods=seq_length, freq='15min')        \n",
    "        for i, time_point in enumerate(time_seq):\n",
    "            # Basic time features\n",
    "            hour = time_point.hour\n",
    "            day_of_year = time_point.dayofyear\n",
    "            month = time_point.month\n",
    "            day_of_week = time_point.dayofweek\n",
    "            \n",
    "            # Cyclical encoding\n",
    "            dummy_data.loc[i, 'hour_sin'] = np.sin(2 * np.pi * hour / 24)\n",
    "            dummy_data.loc[i, 'hour_cos'] = np.cos(2 * np.pi * hour / 24)\n",
    "            dummy_data.loc[i, 'day_sin'] = np.sin(2 * np.pi * day_of_year / 365)\n",
    "            dummy_data.loc[i, 'day_cos'] = np.cos(2 * np.pi * day_of_year / 365)\n",
    "            dummy_data.loc[i, 'month_sin'] = np.sin(2 * np.pi * month / 12)\n",
    "            dummy_data.loc[i, 'month_cos'] = np.cos(2 * np.pi * month / 12)\n",
    "            dummy_data.loc[i, 'dow_sin'] = np.sin(2 * np.pi * day_of_week / 7)\n",
    "            dummy_data.loc[i, 'dow_cos'] = np.cos(2 * np.pi * day_of_week / 7)\n",
    "            \n",
    "            # Solar elevation (simplified)\n",
    "            dummy_data.loc[i, 'solar_elevation'] = max(0, np.sin(2 * np.pi * (hour - 6) / 12) * \n",
    "                                                      np.sin(2 * np.pi * day_of_year / 365))\n",
    "            \n",
    "            # Weekend indicator\n",
    "            dummy_data.loc[i, 'is_weekend'] = int(day_of_week >= 5)\n",
    "            \n",
    "            # Fourier features (simplified)\n",
    "            time_index = i\n",
    "            for j in range(1, 8):\n",
    "                dummy_data.loc[i, f'daily_sin_{j}'] = np.sin(2 * np.pi * j * time_index / 96)\n",
    "                dummy_data.loc[i, f'daily_cos_{j}'] = np.cos(2 * np.pi * j * time_index / 96)\n",
    "            \n",
    "            for j in range(1, 4):\n",
    "                dummy_data.loc[i, f'weekly_sin_{j}'] = np.sin(2 * np.pi * j * time_index / 672)\n",
    "                dummy_data.loc[i, f'weekly_cos_{j}'] = np.cos(2 * np.pi * j * time_index / 672)\n",
    "        \n",
    "        # Add derived features\n",
    "        dummy_data['efficiency'] = 0.9  # Assume typical efficiency\n",
    "        dummy_data['temp_diff'] = module_temp - ambient_temp\n",
    "        dummy_data['irr_temp_ratio'] = irradiation / (module_temp + 1e-8)\n",
    "        dummy_data['power_per_irr'] = current_ac_power / (irradiation + 1e-8)\n",
    "        dummy_data['irr_amb_interaction'] = irradiation * ambient_temp\n",
    "        dummy_data['irr_mod_interaction'] = irradiation * module_temp\n",
    "        dummy_data['temp_interaction'] = ambient_temp * module_temp\n",
    "        \n",
    "        # Add lag and rolling features (simplified - use current values)\n",
    "        for col in ['total_DC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']:\n",
    "            for lag in [1, 2, 4, 8, 16, 24]:\n",
    "                dummy_data[f'{col}_lag_{lag}'] = dummy_data[col].iloc[0]\n",
    "            \n",
    "            for window in [4, 8, 16]:\n",
    "                dummy_data[f'{col}_rolling_mean_{window}'] = dummy_data[col].iloc[0]\n",
    "                dummy_data[f'{col}_rolling_std_{window}'] = 0.1\n",
    "                dummy_data[f'{col}_rolling_max_{window}'] = dummy_data[col].iloc[0] * 1.1\n",
    "                dummy_data[f'{col}_rolling_min_{window}'] = dummy_data[col].iloc[0] * 0.9\n",
    "        \n",
    "        # Select only the features used in training\n",
    "        available_features = [f for f in selected_features if f in dummy_data.columns]\n",
    "        missing_features = [f for f in selected_features if f not in dummy_data.columns]\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features will be set to 0: {missing_features}\")\n",
    "            for f in missing_features:\n",
    "                dummy_data[f] = 0\n",
    "        \n",
    "        # Prepare input sequence\n",
    "        input_data = dummy_data[selected_features].values\n",
    "        input_scaled = feature_scaler.transform(input_data)\n",
    "        input_sequence = input_scaled.reshape(1, seq_length, len(selected_features))\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(input_sequence, verbose=0)\n",
    "        \n",
    "        # Convert predictions back to original scale\n",
    "        results = {}\n",
    "        for i, (h, h_name) in enumerate(zip(prediction_horizons, horizon_names)):\n",
    "            pred_scaled = predictions[i][0][0]\n",
    "            pred_sqrt = target_scaler.inverse_transform([[pred_scaled]])[0][0]\n",
    "            pred_orig = max(0, pred_sqrt ** 2 - 1)  # Inverse of sqrt(x + 1)\n",
    "            \n",
    "            results[h_name] = pred_orig\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return predict_future_power\n",
    "\n",
    "# Evaluate the CV-tuned model\n",
    "results = evaluate_model(final_model, X_test, y_test, target_scaler, prediction_horizons, horizon_names)\n",
    "\n",
    "# Save the CV-tuned model and scalers\n",
    "print(\"\\nSaving CV-tuned model and scalers...\")\n",
    "final_model.save('cv_tuned_bilstm_solar_model.h5')\n",
    "joblib.dump(feature_scaler, 'cv_tuned_feature_scaler.pkl')\n",
    "joblib.dump(target_scaler, 'cv_tuned_target_scaler.pkl')\n",
    "joblib.dump(selected_features, 'cv_tuned_selected_features.pkl')\n",
    "joblib.dump(best_params, 'cv_tuned_best_params.pkl')\n",
    "\n",
    "print(\"CV-tuned model and scalers saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71816ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Analysis\n",
    "def plot_results(results, horizon_names):\n",
    "    \"\"\"Plot prediction results and training history\"\"\"\n",
    "    \n",
    "    # Plot predictions vs actual for each horizon\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, h_name in enumerate(horizon_names):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            y_true = results[h_name]['y_true']\n",
    "            y_pred = results[h_name]['y_pred']\n",
    "            \n",
    "            # Scatter plot\n",
    "            ax.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
    "            \n",
    "            # Perfect prediction line\n",
    "            min_val = min(y_true.min(), y_pred.min())\n",
    "            max_val = max(y_true.max(), y_pred.max())\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "            \n",
    "            ax.set_xlabel('Actual AC Power (kW)')\n",
    "            ax.set_ylabel('Predicted AC Power (kW)')\n",
    "            ax.set_title(f'{h_name.replace(\"_\", \" \").title()} - R² = {results[h_name][\"r2\"]:.3f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot time series for shortest horizon\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    h_name = horizon_names[0]  # 1 hour prediction\n",
    "    y_true = results[h_name]['y_true']\n",
    "    y_pred = results[h_name]['y_pred']\n",
    "    \n",
    "    # Plot subset of data for clarity\n",
    "    n_points = min(500, len(y_true))\n",
    "    indices = np.linspace(0, len(y_true)-1, n_points).astype(int)\n",
    "    \n",
    "    plt.plot(indices, y_true[indices], label='Actual', linewidth=2, alpha=0.8)\n",
    "    plt.plot(indices, y_pred[indices], label='Predicted', linewidth=2, alpha=0.8)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('AC Power (kW)')\n",
    "    plt.title(f'Time Series Prediction - {h_name.replace(\"_\", \" \").title()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Plot MAE for first output (1-hour prediction)\n",
    "    mae_key = f'output_horizon_{prediction_horizons[0]}_mae'\n",
    "    val_mae_key = f'val_output_horizon_{prediction_horizons[0]}_mae'\n",
    "    \n",
    "    if mae_key in history.history:\n",
    "        plt.plot(history.history[mae_key], label='Training MAE')\n",
    "        plt.plot(history.history[val_mae_key], label='Validation MAE')\n",
    "        plt.title('Model MAE (1-hour prediction)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create prediction function with CV-tuned model\n",
    "predict_future = create_prediction_function(final_model, feature_scaler, target_scaler, \n",
    "                                          selected_features, seq_length)\n",
    "\n",
    "# Example prediction with given conditions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRACTICAL PREDICTION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Given conditions\n",
    "current_ac_power = 500  # kW\n",
    "ambient_temp = 27.5     # °C\n",
    "module_temp = 39        # °C\n",
    "irradiation = 0.41      # kW/m²\n",
    "\n",
    "print(f\"\\nCurrent Conditions:\")\n",
    "print(f\"  AC Power: {current_ac_power} kW\")\n",
    "print(f\"  Ambient Temperature: {ambient_temp}°C\")\n",
    "print(f\"  Module Temperature: {module_temp}°C\")\n",
    "print(f\"  Irradiation: {irradiation} kW/m²\")\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = predict_future(current_ac_power, ambient_temp, module_temp, irradiation)\n",
    "\n",
    "print(f\"\\nPredicted Average AC Power:\")\n",
    "for horizon, power in future_predictions.items():\n",
    "    print(f\"  Next {horizon.replace('_', ' ')}: {power:.2f} kW\")\n",
    "\n",
    "# Calculate percentage changes\n",
    "print(f\"\\nPredicted Changes from Current:\")\n",
    "for horizon, power in future_predictions.items():\n",
    "    change = ((power - current_ac_power) / current_ac_power) * 100\n",
    "    print(f\"  Next {horizon.replace('_', ' ')}: {change:+.1f}%\")\n",
    "\n",
    "# Plot results\n",
    "plot_results(results, horizon_names)\n",
    "\n",
    "# Performance Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModel Architecture: Enhanced BiLSTM with Cross-Validation Tuning\")\n",
    "print(f\"Features Used: {len(selected_features)}\")\n",
    "print(f\"Sequence Length: {seq_length} time steps (6 hours)\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "print(f\"\\nPrediction Horizons Performance (CV-Tuned Model):\")\n",
    "for h_name in horizon_names:\n",
    "    r = results[h_name]\n",
    "    print(f\"\\n{h_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"  MAE:  {r['mae']:.2f} kW\")\n",
    "    print(f\"  RMSE: {r['rmse']:.2f} kW\")\n",
    "    print(f\"  MAPE: {r['mape']:.2f}%\")\n",
    "    print(f\"  R²:   {r['r2']:.4f}\")\n",
    "\n",
    "# Model comparison with benchmarks\n",
    "print(f\"\\nModel Improvements with CV and Hyperparameter Tuning:\")\n",
    "print(f\"  ✓ Automated hyperparameter optimization using RandomizedSearchCV\")\n",
    "print(f\"  ✓ Time series cross-validation for robust performance estimation\")\n",
    "print(f\"  ✓ Optimized LSTM units: {best_params.get('lstm_units_1', 64)} → {best_params.get('lstm_units_2', 32)}\")\n",
    "print(f\"  ✓ Optimized dropout rate: {best_params.get('dropout_rate', 0.3)}\")\n",
    "print(f\"  ✓ Optimized learning rate: {best_params.get('learning_rate', 0.0005)}\")\n",
    "print(f\"  ✓ Optimized L2 regularization: {best_params.get('l2_reg', 0.001)}\")\n",
    "print(f\"  ✓ Enhanced temporal feature engineering\")\n",
    "print(f\"  ✓ Multi-horizon prediction capability\")\n",
    "print(f\"  ✓ Physics-based derived features\")\n",
    "print(f\"  ✓ Robust scaling and preprocessing\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - cv_tuned_bilstm_solar_model.h5 (CV-tuned trained model)\")\n",
    "print(f\"  - cv_tuned_feature_scaler.pkl (feature scaler)\")\n",
    "print(f\"  - cv_tuned_target_scaler.pkl (target scaler)\")\n",
    "print(f\"  - cv_tuned_selected_features.pkl (feature list)\")\n",
    "print(f\"  - cv_tuned_best_params.pkl (best hyperparameters)\")\n",
    "if 'cv_summary' in locals():\n",
    "    print(f\"  - cv_evaluation_results.pkl (cross-validation results)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Performance Analysis\n",
    "def perform_final_cv_evaluation(best_params, X_data, y_data, feature_scaler, target_scaler):\n",
    "    \"\"\"Perform final cross-validation evaluation with best parameters\"\"\"\n",
    "    \n",
    "    print(\"\\nPerforming final cross-validation evaluation...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create time series CV\n",
    "    tscv = TimeSeriesCV(n_splits=5, test_size=len(X_data)//6)\n",
    "    \n",
    "    cv_scores = []\n",
    "    cv_predictions = {h_name: [] for h_name in horizon_names}\n",
    "    cv_actuals = {h_name: [] for h_name in horizon_names}\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, test_idx in tscv.split(X_data):\n",
    "        print(f\"\\nFold {fold}/5:\")\n",
    "        print(f\"  Train: {len(train_idx)} samples, Test: {len(test_idx)} samples\")\n",
    "        \n",
    "        # Split data\n",
    "        X_fold_train, X_fold_test = X_data[train_idx], X_data[test_idx]\n",
    "        y_fold_train = {key: val[train_idx] for key, val in y_data.items()}\n",
    "        y_fold_test = {key: val[test_idx] for key, val in y_data.items()}\n",
    "        \n",
    "        # Create and train model\n",
    "        fold_model = MultiHorizonBiLSTM(**best_params)\n",
    "        fold_model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = fold_model.model.predict(X_fold_test, verbose=0)\n",
    "        \n",
    "        # Evaluate each horizon\n",
    "        fold_scores = {}\n",
    "        for i, (h, h_name) in enumerate(zip(prediction_horizons, horizon_names)):\n",
    "            # Get predictions for this horizon\n",
    "            y_pred_scaled = predictions[i].flatten()\n",
    "            y_true_scaled = y_fold_test[f'output_horizon_{h}']\n",
    "            \n",
    "            # Inverse transform\n",
    "            y_pred_sqrt = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "            y_true_sqrt = target_scaler.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Convert to original scale\n",
    "            y_pred_orig = np.maximum(0, y_pred_sqrt ** 2 - 1)\n",
    "            y_true_orig = np.maximum(0, y_true_sqrt ** 2 - 1)\n",
    "            \n",
    "            # Store for overall analysis\n",
    "            cv_predictions[h_name].extend(y_pred_orig)\n",
    "            cv_actuals[h_name].extend(y_true_orig)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "            r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "            \n",
    "            fold_scores[h_name] = {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "            print(f\"  {h_name}: MAE={mae:.2f}, RMSE={rmse:.2f}, R²={r2:.4f}\")\n",
    "        \n",
    "        cv_scores.append(fold_scores)\n",
    "        fold += 1\n",
    "        \n",
    "        # Clear memory\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate overall CV statistics\n",
    "    print(f\"\\nOverall Cross-Validation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    cv_summary = {}\n",
    "    for h_name in horizon_names:\n",
    "        mae_scores = [score[h_name]['mae'] for score in cv_scores]\n",
    "        rmse_scores = [score[h_name]['rmse'] for score in cv_scores]\n",
    "        r2_scores = [score[h_name]['r2'] for score in cv_scores]\n",
    "        \n",
    "        cv_summary[h_name] = {\n",
    "            'mae_mean': np.mean(mae_scores),\n",
    "            'mae_std': np.std(mae_scores),\n",
    "            'rmse_mean': np.mean(rmse_scores),\n",
    "            'rmse_std': np.std(rmse_scores),\n",
    "            'r2_mean': np.mean(r2_scores),\n",
    "            'r2_std': np.std(r2_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{h_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  MAE:  {cv_summary[h_name]['mae_mean']:.2f} ± {cv_summary[h_name]['mae_std']:.2f} kW\")\n",
    "        print(f\"  RMSE: {cv_summary[h_name]['rmse_mean']:.2f} ± {cv_summary[h_name]['rmse_std']:.2f} kW\")\n",
    "        print(f\"  R²:   {cv_summary[h_name]['r2_mean']:.4f} ± {cv_summary[h_name]['r2_std']:.4f}\")\n",
    "    \n",
    "    return cv_summary, cv_predictions, cv_actuals\n",
    "\n",
    "def plot_cv_results(cv_summary, cv_predictions, cv_actuals):\n",
    "    \"\"\"Plot cross-validation results\"\"\"\n",
    "    \n",
    "    # Plot CV performance comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: MAE comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    mae_means = [cv_summary[h]['mae_mean'] for h in horizon_names]\n",
    "    mae_stds = [cv_summary[h]['mae_std'] for h in horizon_names]\n",
    "    x_pos = range(len(horizon_names))\n",
    "    \n",
    "    bars = ax1.bar(x_pos, mae_means, yerr=mae_stds, capsize=5, alpha=0.7)\n",
    "    ax1.set_xlabel('Prediction Horizon')\n",
    "    ax1.set_ylabel('MAE (kW)')\n",
    "    ax1.set_title('Cross-Validation MAE Performance')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels([h.replace('_', ' ').title() for h in horizon_names], rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mae in zip(bars, mae_means):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{mae:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: R² comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    r2_means = [cv_summary[h]['r2_mean'] for h in horizon_names]\n",
    "    r2_stds = [cv_summary[h]['r2_std'] for h in horizon_names]\n",
    "    \n",
    "    bars = ax2.bar(x_pos, r2_means, yerr=r2_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    ax2.set_xlabel('Prediction Horizon')\n",
    "    ax2.set_ylabel('R² Score')\n",
    "    ax2.set_title('Cross-Validation R² Performance')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels([h.replace('_', ' ').title() for h in horizon_names], rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, r2 in zip(bars, r2_means):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{r2:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Prediction vs Actual scatter (1-hour horizon)\n",
    "    ax3 = axes[1, 0]\n",
    "    h_name = horizon_names[0]  # 1-hour predictions\n",
    "    y_true = np.array(cv_actuals[h_name])\n",
    "    y_pred = np.array(cv_predictions[h_name])\n",
    "    \n",
    "    ax3.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    ax3.set_xlabel('Actual AC Power (kW)')\n",
    "    ax3.set_ylabel('Predicted AC Power (kW)')\n",
    "    ax3.set_title(f'CV Predictions vs Actual - {h_name.replace(\"_\", \" \").title()}')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    errors = y_pred - y_true\n",
    "    ax4.hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax4.set_xlabel('Prediction Error (kW)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title(f'Error Distribution - {h_name.replace(\"_\", \" \").title()}')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Combine training and test data for comprehensive CV evaluation\n",
    "X_full = np.concatenate([X_train, X_test], axis=0)\n",
    "y_full = {}\n",
    "for key in y_train.keys():\n",
    "    y_full[key] = np.concatenate([y_train[key], y_test[key]], axis=0)\n",
    "\n",
    "print(f\"\\nFull dataset for CV: {X_full.shape[0]} samples\")\n",
    "\n",
    "# Perform comprehensive CV evaluation if hyperparameter tuning was performed\n",
    "if perform_tuning and 'search_results' in locals():\n",
    "    cv_summary, cv_predictions, cv_actuals = perform_final_cv_evaluation(\n",
    "        best_params, X_full, y_full, feature_scaler, target_scaler\n",
    "    )\n",
    "    \n",
    "    # Plot CV results\n",
    "    plot_cv_results(cv_summary, cv_predictions, cv_actuals)\n",
    "    \n",
    "    # Save CV results\n",
    "    joblib.dump(cv_summary, 'cv_evaluation_results.pkl')\n",
    "    print(\"\\nCV evaluation results saved to 'cv_evaluation_results.pkl'\")\n",
    "else:\n",
    "    print(\"\\nSkipping comprehensive CV evaluation (hyperparameter tuning was not performed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35478f",
   "metadata": {},
   "source": [
    "# Enhanced BiLSTM Model with Cross-Validation and Hyperparameter Tuning\n",
    "\n",
    "## Key Improvements for Optimal Performance:\n",
    "\n",
    "### 1. **Cross-Validation and Hyperparameter Tuning**\n",
    "- **Implemented RandomizedSearchCV** with time series cross-validation for robust hyperparameter optimization\n",
    "- **Custom TimeSeriesCV class** respects temporal order and prevents data leakage\n",
    "- **Automated parameter search** across LSTM units, dropout rates, learning rates, and regularization\n",
    "- **20 random combinations tested** from comprehensive parameter grid for efficiency\n",
    "- **5-fold time series CV** for reliable performance estimation\n",
    "\n",
    "### 2. **Optimized Model Architecture**\n",
    "- **Data-driven hyperparameter selection** instead of manual tuning\n",
    "- **Simplified BiLSTM layers** with optimal units determined by CV (typically 64→32)\n",
    "- **Proper L2 regularization** with CV-optimized strength\n",
    "- **Optimized dropout rates** for each layer based on validation performance\n",
    "- **Multi-horizon outputs** with shared representation for efficiency\n",
    "\n",
    "### 3. **Robust Training Process**\n",
    "- **CV-validated learning rate** for stable convergence\n",
    "- **Optimized batch size** based on validation performance\n",
    "- **Huber loss function** for robustness to outliers\n",
    "- **Enhanced early stopping** with optimal patience values\n",
    "- **Gradient clipping** to prevent exploding gradients\n",
    "\n",
    "### 4. **Advanced Validation Strategy**\n",
    "- **Time series cross-validation** preserves temporal dependencies\n",
    "- **Comprehensive performance metrics** across all prediction horizons\n",
    "- **Statistical significance testing** with confidence intervals\n",
    "- **Error distribution analysis** for model reliability assessment\n",
    "\n",
    "### 5. **Enhanced Feature Engineering**\n",
    "- **Reduced feature set** (~50 features) optimized through validation\n",
    "- **Physics-based derived features** for better model interpretability\n",
    "- **Temporal encoding** with cyclical features for seasonality\n",
    "- **Fourier components** for capturing periodic patterns\n",
    "- **Lag and rolling features** for temporal dependencies\n",
    "\n",
    "### 6. **Improved Data Preprocessing**\n",
    "- **RobustScaler for targets** to handle outliers effectively\n",
    "- **MinMaxScaler for features** with symmetric range (-1,1)\n",
    "- **Square root transformation** for better target distribution\n",
    "- **Chronological data splitting** to prevent temporal leakage\n",
    "\n",
    "### Expected Benefits:\n",
    "- **Significantly improved generalization** through CV-based validation\n",
    "- **Optimal hyperparameters** for the specific dataset and task\n",
    "- **Reduced overfitting risk** with data-driven parameter selection\n",
    "- **More reliable performance estimates** with cross-validation\n",
    "- **Better prediction accuracy** especially for multi-horizon forecasting\n",
    "- **Robust model performance** across different time periods\n",
    "\n",
    "### Model Performance:\n",
    "- **Automated optimization** ensures best possible performance for the given architecture\n",
    "- **Statistical validation** provides confidence in model reliability\n",
    "- **Multi-fold evaluation** reduces variance in performance estimates\n",
    "- **Time series specific validation** respects temporal nature of data\n",
    "\n",
    "### Files Generated:\n",
    "- `cv_tuned_bilstm_solar_model.h5` - CV-optimized model\n",
    "- `cv_tuned_best_params.pkl` - Optimal hyperparameters\n",
    "- `cv_evaluation_results.pkl` - Cross-validation performance metrics\n",
    "- All preprocessing components for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
