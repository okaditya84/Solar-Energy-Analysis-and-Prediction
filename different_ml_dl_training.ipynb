{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2RrTXfNctnx"
      },
      "source": [
        "# Different ML/DL models with ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-O98UYAH_7YS",
        "outputId": "ae00d972-7be5-428b-ccb8-da23f5a8d11a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization, GRU, Conv1D, MaxPooling1D, Flatten, Input, Add, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SolarPowerForecaster:\n",
        "    def __init__(self, model_save_dir='saved_models'):\n",
        "        self.scalers = {}\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.model_save_dir = model_save_dir\n",
        "        self.feature_cols = None\n",
        "        self.ensemble_weights = {}\n",
        "\n",
        "        # Create save directory if it doesn't exist\n",
        "        if not os.path.exists(model_save_dir):\n",
        "            os.makedirs(model_save_dir)\n",
        "\n",
        "    def load_and_preprocess_data(self, gen_path, weather_path):\n",
        "        \"\"\"Load and preprocess both generation and weather data\"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load generation data\n",
        "        gen_data = pd.read_csv(gen_path)\n",
        "\n",
        "        # Robust DATE_TIME parsing with fallbacks\n",
        "        gen_dates = pd.to_datetime(gen_data['DATE_TIME'], infer_datetime_format=True, errors='coerce')\n",
        "        if gen_dates.isna().any():\n",
        "            # Try common alternative formats\n",
        "            alt1 = pd.to_datetime(gen_data['DATE_TIME'], format='%d-%m-%Y %H:%M', errors='coerce')\n",
        "            alt2 = pd.to_datetime(gen_data['DATE_TIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "            gen_dates = gen_dates.fillna(alt1).fillna(alt2)\n",
        "        if gen_dates.isna().any():\n",
        "            bad_samples = gen_data['DATE_TIME'][gen_dates.isna()].unique()[:5]\n",
        "            raise ValueError(f\"Couldn't parse some DATE_TIME entries in generation data. Examples: {bad_samples}\")\n",
        "        gen_data['DATE_TIME'] = gen_dates\n",
        "\n",
        "        # Aggregate AC_POWER across inverters and calculate additional features\n",
        "        gen_agg = gen_data.groupby('DATE_TIME').agg({\n",
        "            'AC_POWER': ['sum', 'mean', 'std', 'min', 'max'],\n",
        "            'DC_POWER': ['sum', 'mean', 'std'],\n",
        "            'DAILY_YIELD': 'mean',\n",
        "            'TOTAL_YIELD': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        gen_agg.columns = ['DATE_TIME', 'total_AC_POWER', 'mean_AC_POWER', 'std_AC_POWER',\n",
        "                          'min_AC_POWER', 'max_AC_POWER', 'total_DC_POWER', 'mean_DC_POWER',\n",
        "                          'std_DC_POWER', 'mean_DAILY_YIELD', 'mean_TOTAL_YIELD']\n",
        "\n",
        "        # Load weather data\n",
        "        weather_data = pd.read_csv(weather_path)\n",
        "\n",
        "        weather_dates = pd.to_datetime(weather_data['DATE_TIME'], infer_datetime_format=True, errors='coerce')\n",
        "        if weather_dates.isna().any():\n",
        "            alt1 = pd.to_datetime(weather_data['DATE_TIME'], format='%d-%m-%Y %H:%M', errors='coerce')\n",
        "            alt2 = pd.to_datetime(weather_data['DATE_TIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "            weather_dates = weather_dates.fillna(alt1).fillna(alt2)\n",
        "        if weather_dates.isna().any():\n",
        "            bad_samples = weather_data['DATE_TIME'][weather_dates.isna()].unique()[:5]\n",
        "            raise ValueError(f\"Couldn't parse some DATE_TIME entries in weather data. Examples: {bad_samples}\")\n",
        "        weather_data['DATE_TIME'] = weather_dates\n",
        "\n",
        "        # Merge datasets\n",
        "        data = pd.merge(gen_agg, weather_data[['DATE_TIME', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']],\n",
        "                       on='DATE_TIME', how='inner')\n",
        "\n",
        "        # Fill missing values\n",
        "        data = data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def save_all_models(self, scaler, feature_cols, ensemble_result=None):\n",
        "        \"\"\"Save all trained models, scaler, and metadata\"\"\"\n",
        "        print(f\"Saving all models to {self.model_save_dir}...\")\n",
        "\n",
        "        # Save timestamp for this training session\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save scaler\n",
        "        scaler_path = os.path.join(self.model_save_dir, f'scaler_{timestamp}.pkl')\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "        # Save feature columns\n",
        "        feature_path = os.path.join(self.model_save_dir, f'feature_cols_{timestamp}.pkl')\n",
        "        with open(feature_path, 'wb') as f:\n",
        "            pickle.dump(feature_cols, f)\n",
        "        print(f\"Feature columns saved to: {feature_path}\")\n",
        "\n",
        "        # Save each model\n",
        "        saved_models_info = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if name in ['Transformer', 'CNN_LSTM', 'GRU_Attention']:\n",
        "                # Save Keras/TensorFlow models\n",
        "                model_path = os.path.join(self.model_save_dir, f'{name}_{timestamp}.h5')\n",
        "                model.save(model_path)\n",
        "                saved_models_info[name] = {'path': model_path, 'type': 'keras'}\n",
        "                print(f\"{name} model saved to: {model_path}\")\n",
        "            else:\n",
        "                # Save sklearn/xgboost/lightgbm models\n",
        "                model_path = os.path.join(self.model_save_dir, f'{name}_{timestamp}.pkl')\n",
        "                joblib.dump(model, model_path)\n",
        "                saved_models_info[name] = {'path': model_path, 'type': 'sklearn'}\n",
        "                print(f\"{name} model saved to: {model_path}\")\n",
        "\n",
        "        # Save ensemble weights if available\n",
        "        if ensemble_result and 'weights' in ensemble_result:\n",
        "            weights_path = os.path.join(self.model_save_dir, f'ensemble_weights_{timestamp}.json')\n",
        "            with open(weights_path, 'w') as f:\n",
        "                json.dump(ensemble_result['weights'], f, indent=2)\n",
        "            print(f\"Ensemble weights saved to: {weights_path}\")\n",
        "            saved_models_info['ensemble_weights'] = weights_path\n",
        "\n",
        "        # Save results if available\n",
        "        if self.results:\n",
        "            results_path = os.path.join(self.model_save_dir, f'results_{timestamp}.pkl')\n",
        "            with open(results_path, 'wb') as f:\n",
        "                pickle.dump(self.results, f)\n",
        "            print(f\"Results saved to: {results_path}\")\n",
        "            saved_models_info['results'] = results_path\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'timestamp': timestamp,\n",
        "            'scaler_path': scaler_path,\n",
        "            'feature_cols_path': feature_path,\n",
        "            'models': saved_models_info,\n",
        "            'n_features': len(feature_cols)\n",
        "        }\n",
        "\n",
        "        metadata_path = os.path.join(self.model_save_dir, f'metadata_{timestamp}.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"Metadata saved to: {metadata_path}\")\n",
        "\n",
        "        # Save latest metadata (for easy loading)\n",
        "        latest_metadata_path = os.path.join(self.model_save_dir, 'latest_metadata.json')\n",
        "        with open(latest_metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"Latest metadata saved to: {latest_metadata_path}\")\n",
        "\n",
        "        print(f\"All models and components saved successfully with timestamp: {timestamp}\")\n",
        "        return metadata_path\n",
        "\n",
        "    def load_all_models(self, metadata_path=None):\n",
        "        \"\"\"Load all models from saved files\"\"\"\n",
        "        if metadata_path is None:\n",
        "            metadata_path = os.path.join(self.model_save_dir, 'latest_metadata.json')\n",
        "\n",
        "        print(f\"Loading models from metadata: {metadata_path}\")\n",
        "\n",
        "        # Load metadata\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        # Load scaler\n",
        "        scaler = joblib.load(metadata['scaler_path'])\n",
        "        print(f\"Scaler loaded from: {metadata['scaler_path']}\")\n",
        "\n",
        "        # Load feature columns\n",
        "        with open(metadata['feature_cols_path'], 'rb') as f:\n",
        "            feature_cols = pickle.load(f)\n",
        "        print(f\"Feature columns loaded from: {metadata['feature_cols_path']}\")\n",
        "\n",
        "        # Load models\n",
        "        self.models = {}\n",
        "        for name, model_info in metadata['models'].items():\n",
        "            if name in ['ensemble_weights', 'results']:\n",
        "                continue\n",
        "\n",
        "            if model_info['type'] == 'keras':\n",
        "                model = load_model(model_info['path'])\n",
        "                print(f\"{name} (Keras) model loaded from: {model_info['path']}\")\n",
        "            else:\n",
        "                model = joblib.load(model_info['path'])\n",
        "                print(f\"{name} (Sklearn) model loaded from: {model_info['path']}\")\n",
        "\n",
        "            self.models[name] = model\n",
        "\n",
        "        # Load ensemble weights if available\n",
        "        if 'ensemble_weights' in metadata['models']:\n",
        "            with open(metadata['models']['ensemble_weights'], 'r') as f:\n",
        "                self.ensemble_weights = json.load(f)\n",
        "            print(f\"Ensemble weights loaded from: {metadata['models']['ensemble_weights']}\")\n",
        "\n",
        "        # Load results if available\n",
        "        if 'results' in metadata['models']:\n",
        "            with open(metadata['models']['results'], 'rb') as f:\n",
        "                self.results = pickle.load(f)\n",
        "            print(f\"Results loaded from: {metadata['models']['results']}\")\n",
        "\n",
        "        self.feature_cols = feature_cols\n",
        "        print(f\"Successfully loaded {len(self.models)} models\")\n",
        "\n",
        "        return scaler, feature_cols\n",
        "\n",
        "    def predict_with_loaded_models(self, X_test, scaler, model_type='all'):\n",
        "        \"\"\"Make predictions using loaded models\"\"\"\n",
        "        if not self.models:\n",
        "            raise ValueError(\"No models loaded. Please load models first using load_all_models()\")\n",
        "\n",
        "        predictions = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            try:\n",
        "                if name in ['Transformer', 'CNN_LSTM', 'GRU_Attention']:\n",
        "                    if model_type in ['deep', 'all']:\n",
        "                        y_pred = model.predict(X_test, verbose=0)\n",
        "                        predictions[name] = y_pred.flatten()\n",
        "                else:\n",
        "                    if model_type in ['ensemble', 'all']:\n",
        "                        # For ensemble models, reshape X_test to 2D\n",
        "                        X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
        "                        y_pred = model.predict(X_test_2d)\n",
        "                        predictions[name] = y_pred.flatten()\n",
        "            except Exception as e:\n",
        "                print(f\"Error predicting with {name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def create_ensemble_prediction_from_loaded(self, predictions, actual_values=None):\n",
        "        \"\"\"Create ensemble prediction using loaded weights\"\"\"\n",
        "        if not self.ensemble_weights:\n",
        "            print(\"No ensemble weights found. Creating equal weight ensemble...\")\n",
        "            # Create equal weights\n",
        "            weights = {name: 1.0/len(predictions) for name in predictions.keys()}\n",
        "        else:\n",
        "            weights = self.ensemble_weights\n",
        "            # Filter weights for available predictions\n",
        "            weights = {name: weight for name, weight in weights.items() if name in predictions}\n",
        "            # Normalize weights\n",
        "            total_weight = sum(weights.values())\n",
        "            weights = {name: weight/total_weight for name, weight in weights.items()}\n",
        "\n",
        "        # Create weighted ensemble\n",
        "        ensemble_pred = np.zeros_like(list(predictions.values())[0])\n",
        "\n",
        "        for name, pred in predictions.items():\n",
        "            if name in weights:\n",
        "                ensemble_pred += weights[name] * pred\n",
        "\n",
        "        # Calculate metrics if actual values provided\n",
        "        if actual_values is not None:\n",
        "            mae = mean_absolute_error(actual_values, ensemble_pred)\n",
        "            rmse = np.sqrt(mean_squared_error(actual_values, ensemble_pred))\n",
        "            mape = np.mean(np.abs((actual_values - ensemble_pred) / (actual_values + 1e-10))) * 100\n",
        "            r2 = r2_score(actual_values, ensemble_pred)\n",
        "\n",
        "            print(f'Ensemble - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}%, R²: {r2:.4f}')\n",
        "\n",
        "            return ensemble_pred, {\n",
        "                'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2,\n",
        "                'weights': weights\n",
        "            }\n",
        "\n",
        "        return ensemble_pred, {'weights': weights}\n",
        "\n",
        "    def list_saved_models(self):\n",
        "        \"\"\"List all saved model sessions\"\"\"\n",
        "        if not os.path.exists(self.model_save_dir):\n",
        "            print(f\"No saved models directory found: {self.model_save_dir}\")\n",
        "            return []\n",
        "\n",
        "        metadata_files = [f for f in os.listdir(self.model_save_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
        "\n",
        "        print(f\"Found {len(metadata_files)} saved model sessions:\")\n",
        "        sessions = []\n",
        "\n",
        "        for metadata_file in sorted(metadata_files):\n",
        "            metadata_path = os.path.join(self.model_save_dir, metadata_file)\n",
        "            try:\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "                timestamp = metadata['timestamp']\n",
        "                n_models = len([k for k in metadata['models'].keys() if k not in ['ensemble_weights', 'results']])\n",
        "                print(f\"  - {timestamp}: {n_models} models\")\n",
        "                sessions.append({'timestamp': timestamp, 'path': metadata_path, 'n_models': n_models})\n",
        "            except Exception as e:\n",
        "                print(f\"  - Error reading {metadata_file}: {str(e)}\")\n",
        "\n",
        "        return sessions\n",
        "\n",
        "    def create_advanced_features(self, data):\n",
        "        \"\"\"Create comprehensive feature engineering\"\"\"\n",
        "        print(\"Creating advanced features...\")\n",
        "\n",
        "        # Time-based features\n",
        "        data['hour'] = data['DATE_TIME'].dt.hour\n",
        "        data['day'] = data['DATE_TIME'].dt.day\n",
        "        data['month'] = data['DATE_TIME'].dt.month\n",
        "        data['day_of_year'] = data['DATE_TIME'].dt.dayofyear\n",
        "        data['week_of_year'] = data['DATE_TIME'].dt.isocalendar().week\n",
        "        data['is_weekend'] = data['DATE_TIME'].dt.weekday >= 5\n",
        "\n",
        "        # Cyclical encoding\n",
        "        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
        "        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
        "        data['day_sin'] = np.sin(2 * np.pi * data['day'] / 31)\n",
        "        data['day_cos'] = np.cos(2 * np.pi * data['day'] / 31)\n",
        "        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
        "        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
        "        data['doy_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365)\n",
        "        data['doy_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365)\n",
        "\n",
        "        # Weather-based features\n",
        "        data['temp_diff'] = data['MODULE_TEMPERATURE'] - data['AMBIENT_TEMPERATURE']\n",
        "        data['temp_ratio'] = data['MODULE_TEMPERATURE'] / (data['AMBIENT_TEMPERATURE'] + 1e-8)\n",
        "        data['irradiation_temp_interaction'] = data['IRRADIATION'] * data['MODULE_TEMPERATURE']\n",
        "        data['power_efficiency'] = data['total_AC_POWER'] / (data['total_DC_POWER'] + 1e-8)\n",
        "\n",
        "        # Lag features\n",
        "        for lag in [1, 2, 3, 6, 12, 24]:\n",
        "            data[f'AC_POWER_lag_{lag}'] = data['total_AC_POWER'].shift(lag)\n",
        "            data[f'IRRADIATION_lag_{lag}'] = data['IRRADIATION'].shift(lag)\n",
        "            data[f'MODULE_TEMP_lag_{lag}'] = data['MODULE_TEMPERATURE'].shift(lag)\n",
        "\n",
        "        # Rolling window features\n",
        "        for window in [3, 6, 12, 24]:\n",
        "            data[f'AC_POWER_rolling_mean_{window}'] = data['total_AC_POWER'].rolling(window=window).mean()\n",
        "            data[f'AC_POWER_rolling_std_{window}'] = data['total_AC_POWER'].rolling(window=window).std()\n",
        "            data[f'IRRADIATION_rolling_mean_{window}'] = data['IRRADIATION'].rolling(window=window).mean()\n",
        "            data[f'MODULE_TEMP_rolling_mean_{window}'] = data['MODULE_TEMPERATURE'].rolling(window=window).mean()\n",
        "\n",
        "        # Fourier features for multiple periodicities\n",
        "        self._add_fourier_features(data, period=96, n_terms=8, prefix='daily')  # Daily pattern\n",
        "        self._add_fourier_features(data, period=672, n_terms=4, prefix='weekly')  # Weekly pattern\n",
        "\n",
        "        # Clear sky model approximation\n",
        "        data['clear_sky_power'] = self._calculate_clear_sky_power(data)\n",
        "        data['power_clear_sky_ratio'] = data['total_AC_POWER'] / (data['clear_sky_power'] + 1e-8)\n",
        "\n",
        "        # Remove rows with NaN values created by lag and rolling features\n",
        "        data = data.dropna().reset_index(drop=True)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _add_fourier_features(self, data, period, n_terms, prefix):\n",
        "        \"\"\"Add Fourier features for specific periodicity\"\"\"\n",
        "        data['time_index'] = range(len(data))\n",
        "        for i in range(1, n_terms + 1):\n",
        "            data[f'{prefix}_sin_{i}'] = np.sin(2 * np.pi * i * data['time_index'] / period)\n",
        "            data[f'{prefix}_cos_{i}'] = np.cos(2 * np.pi * i * data['time_index'] / period)\n",
        "        data.drop('time_index', axis=1, inplace=True)\n",
        "\n",
        "    def _calculate_clear_sky_power(self, data):\n",
        "        \"\"\"Simple clear sky power estimation\"\"\"\n",
        "        # Simplified clear sky model based on solar elevation\n",
        "        solar_elevation = np.maximum(0, np.sin(2 * np.pi * (data['hour'] - 6) / 12))\n",
        "        clear_sky_irradiance = 1000 * solar_elevation  # Simplified\n",
        "        return clear_sky_irradiance * 0.2  # Simplified efficiency factor\n",
        "\n",
        "    def prepare_sequences(self, data, features, target, seq_length=48):\n",
        "        \"\"\"Prepare sequences for time series models\"\"\"\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            X.append(data[features].iloc[i:i + seq_length].values)\n",
        "            y.append(data[target].iloc[i + seq_length])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def build_transformer_model(self, seq_length, n_features):\n",
        "        \"\"\"Build a Transformer-based model\"\"\"\n",
        "        inputs = Input(shape=(seq_length, n_features))\n",
        "\n",
        "        # Multi-head attention layers\n",
        "        attn1 = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(inputs, inputs)\n",
        "        attn1 = LayerNormalization()(attn1 + inputs)\n",
        "\n",
        "        attn2 = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1)(attn1, attn1)\n",
        "        attn2 = LayerNormalization()(attn2 + attn1)\n",
        "\n",
        "        # Feed forward network\n",
        "        ffn = Dense(256, activation='relu')(attn2)\n",
        "        ffn = Dropout(0.1)(ffn)\n",
        "        ffn = Dense(n_features)(ffn)\n",
        "        ffn = LayerNormalization()(ffn + attn2)\n",
        "\n",
        "        # Global average pooling and output\n",
        "        x = tf.keras.layers.GlobalAveragePooling1D()(ffn)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1)(x)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "        return model\n",
        "\n",
        "    def build_cnn_lstm_model(self, seq_length, n_features):\n",
        "        \"\"\"Build a CNN-LSTM hybrid model\"\"\"\n",
        "        model = Sequential([\n",
        "            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_length, n_features)),\n",
        "            BatchNormalization(),\n",
        "            Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            LSTM(100, return_sequences=True, dropout=0.2),\n",
        "            LSTM(50, dropout=0.2),\n",
        "\n",
        "            Dense(128, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def build_gru_attention_model(self, seq_length, n_features):\n",
        "        \"\"\"Build a GRU model with attention mechanism\"\"\"\n",
        "        inputs = Input(shape=(seq_length, n_features))\n",
        "\n",
        "        # GRU layers\n",
        "        gru1 = GRU(128, return_sequences=True, dropout=0.2)(inputs)\n",
        "        gru2 = GRU(64, return_sequences=True, dropout=0.2)(gru1)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention = MultiHeadAttention(num_heads=4, key_dim=32)(gru2, gru2)\n",
        "        attention = LayerNormalization()(attention + gru2)\n",
        "\n",
        "        # Global pooling and dense layers\n",
        "        x = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1)(x)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "        return model\n",
        "\n",
        "    def train_deep_models(self, X_train, y_train, X_val, y_val, seq_length, n_features):\n",
        "        \"\"\"Train multiple deep learning models\"\"\"\n",
        "        print(\"Training deep learning models...\")\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7)\n",
        "\n",
        "        models_config = {\n",
        "            'Transformer': self.build_transformer_model(seq_length, n_features),\n",
        "            'CNN_LSTM': self.build_cnn_lstm_model(seq_length, n_features),\n",
        "            'GRU_Attention': self.build_gru_attention_model(seq_length, n_features)\n",
        "        }\n",
        "\n",
        "        for name, model in models_config.items():\n",
        "            print(f\"Training {name}...\")\n",
        "\n",
        "            # Compile with different optimizers for different models\n",
        "            if name == 'Transformer':\n",
        "                optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "            else:\n",
        "                optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "            # Train the model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=100,\n",
        "                batch_size=64,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=[early_stopping, reduce_lr],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            self.models[name] = model\n",
        "\n",
        "    def train_ensemble_models(self, X_train, y_train):\n",
        "        \"\"\"Train ensemble models\"\"\"\n",
        "        print(\"Training ensemble models...\")\n",
        "\n",
        "        ensemble_models = {\n",
        "            'XGBoost': xgb.XGBRegressor(\n",
        "                n_estimators=1000,\n",
        "                max_depth=8,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'LightGBM': lgb.LGBMRegressor(\n",
        "                n_estimators=1000,\n",
        "                max_depth=8,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                verbose=-1\n",
        "            ),\n",
        "            'RandomForest': RandomForestRegressor(\n",
        "                n_estimators=500,\n",
        "                max_depth=15,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            'GradientBoosting': GradientBoostingRegressor(\n",
        "                n_estimators=500,\n",
        "                max_depth=8,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42\n",
        "            )\n",
        "        }\n",
        "\n",
        "        for name, model in ensemble_models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            self.models[name] = model\n",
        "\n",
        "    def predict_and_evaluate(self, X_test, y_test, scaler, model_type='deep'):\n",
        "        \"\"\"Make predictions and evaluate models\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if model_type == 'deep' and name in ['Transformer', 'CNN_LSTM', 'GRU_Attention']:\n",
        "                y_pred = model.predict(X_test, verbose=0)\n",
        "            elif model_type == 'ensemble' and name not in ['Transformer', 'CNN_LSTM', 'GRU_Attention']:\n",
        "                # For ensemble models, we need to reshape X_test\n",
        "                X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
        "                y_pred = model.predict(X_test_2d)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            predictions[name] = y_pred.flatten()\n",
        "\n",
        "        # Inverse transform predictions\n",
        "        results = {}\n",
        "        # assume scaler was fit with features + target where target was the last column\n",
        "        target_idx = scaler.n_features_in_ - 1 if hasattr(scaler, 'n_features_in_') else -1\n",
        "\n",
        "        for name, y_pred in predictions.items():\n",
        "            # Create dummy array for inverse transformation (target at last column)\n",
        "            dummy_array = np.zeros((len(y_pred), scaler.n_features_in_))\n",
        "            dummy_array[:, target_idx] = y_pred\n",
        "            y_pred_rescaled = scaler.inverse_transform(dummy_array)[:, target_idx]\n",
        "\n",
        "            # Do the same for actual values\n",
        "            dummy_array_actual = np.zeros((len(y_test), scaler.n_features_in_))\n",
        "            dummy_array_actual[:, target_idx] = y_test\n",
        "            y_test_rescaled = scaler.inverse_transform(dummy_array_actual)[:, target_idx]\n",
        "\n",
        "            # Calculate metrics\n",
        "            mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
        "            mape = np.mean(np.abs((y_test_rescaled - y_pred_rescaled) / (y_test_rescaled + 1e-10))) * 100\n",
        "            r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "\n",
        "            results[name] = {\n",
        "                'MAE': mae,\n",
        "                'RMSE': rmse,\n",
        "                'MAPE': mape,\n",
        "                'R2': r2,\n",
        "                'predictions': y_pred_rescaled,\n",
        "                'actual': y_test_rescaled\n",
        "            }\n",
        "\n",
        "            print(f'{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}%, R²: {r2:.4f}')\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_ensemble_prediction(self, results):\n",
        "        \"\"\"Create ensemble prediction from multiple models\"\"\"\n",
        "        print(\"Creating ensemble prediction...\")\n",
        "\n",
        "        # Weight models based on their performance (inverse of MAE)\n",
        "        weights = {}\n",
        "        total_weight = 0\n",
        "\n",
        "        for name, result in results.items():\n",
        "            weight = 1.0 / (result['MAE'] + 1e-10)\n",
        "            weights[name] = weight\n",
        "            total_weight += weight\n",
        "\n",
        "        # Normalize weights\n",
        "        for name in weights:\n",
        "            weights[name] /= total_weight\n",
        "\n",
        "        # Create weighted ensemble\n",
        "        ensemble_pred = np.zeros_like(list(results.values())[0]['predictions'])\n",
        "        actual = list(results.values())[0]['actual']\n",
        "\n",
        "        for name, result in results.items():\n",
        "            ensemble_pred += weights[name] * result['predictions']\n",
        "\n",
        "        # Calculate ensemble metrics\n",
        "        mae = mean_absolute_error(actual, ensemble_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(actual, ensemble_pred))\n",
        "        mape = np.mean(np.abs((actual - ensemble_pred) / (actual + 1e-10))) * 100\n",
        "        r2 = r2_score(actual, ensemble_pred)\n",
        "\n",
        "        ensemble_result = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'R2': r2,\n",
        "            'predictions': ensemble_pred,\n",
        "            'actual': actual,\n",
        "            'weights': weights\n",
        "        }\n",
        "\n",
        "        print(f'Ensemble - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}%, R²: {r2:.4f}')\n",
        "        print(f'Model weights: {weights}')\n",
        "\n",
        "        return ensemble_result\n",
        "\n",
        "    def plot_results(self, results, ensemble_result=None):\n",
        "        \"\"\"Plot prediction results\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "\n",
        "        # Plot individual model results\n",
        "        for i, (name, result) in enumerate(results.items()):\n",
        "            if i >= 4:  # Only plot first 4 models\n",
        "                break\n",
        "            row, col = i // 2, i % 2\n",
        "\n",
        "            axes[row, col].plot(result['actual'][:200], label='Actual', alpha=0.7)\n",
        "            axes[row, col].plot(result['predictions'][:200], label='Predicted', alpha=0.7)\n",
        "            axes[row, col].set_title(f'{name} - MAE: {result[\"MAE\"]:.4f}')\n",
        "            axes[row, col].set_xlabel('Time Step')\n",
        "            axes[row, col].set_ylabel('AC Power (kW)')\n",
        "            axes[row, col].legend()\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ensemble result if available\n",
        "        if ensemble_result:\n",
        "            plt.figure(figsize=(15, 8))\n",
        "            plt.plot(ensemble_result['actual'][:200], label='Actual', alpha=0.8, linewidth=2)\n",
        "            plt.plot(ensemble_result['predictions'][:200], label='Ensemble Prediction', alpha=0.8, linewidth=2)\n",
        "            plt.title(f'Ensemble Model - MAE: {ensemble_result[\"MAE\"]:.4f}, RMSE: {ensemble_result[\"RMSE\"]:.4f}')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('AC Power (kW)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    # Initialize forecaster\n",
        "    forecaster = SolarPowerForecaster()\n",
        "\n",
        "    # Load and preprocess data\n",
        "    gen_path = 'Plant1_filtered.csv'\n",
        "    weather_path = 'Plant1_Weather_filtered.csv'\n",
        "\n",
        "    data = forecaster.load_and_preprocess_data(gen_path, weather_path)\n",
        "    data = forecaster.create_advanced_features(data)\n",
        "\n",
        "    # Define features and target\n",
        "    feature_cols = [col for col in data.columns if col not in ['DATE_TIME', 'total_AC_POWER']]\n",
        "    target_col = 'total_AC_POWER'\n",
        "\n",
        "    print(f\"Total features: {len(feature_cols)}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = RobustScaler()  # Using RobustScaler as it's less sensitive to outliers\n",
        "    # Fit scaler on features + target where target is last column\n",
        "    scaled_features = scaler.fit_transform(data[feature_cols + [target_col]])\n",
        "\n",
        "    # Create scaled dataframe\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols + [target_col])\n",
        "    scaled_df['DATE_TIME'] = data['DATE_TIME'].values\n",
        "\n",
        "    # Prepare sequences for deep learning models\n",
        "    seq_length = 48  # 12 hours\n",
        "    X_seq, y_seq = forecaster.prepare_sequences(scaled_df, feature_cols, target_col, seq_length)\n",
        "\n",
        "    # Split data chronologically\n",
        "    train_size = int(0.7 * len(X_seq))\n",
        "    val_size = int(0.15 * len(X_seq))\n",
        "\n",
        "    X_train_seq = X_seq[:train_size]\n",
        "    y_train_seq = y_seq[:train_size]\n",
        "    X_val_seq = X_seq[train_size:train_size + val_size]\n",
        "    y_val_seq = y_seq[train_size:train_size + val_size]\n",
        "    X_test_seq = X_seq[train_size + val_size:]\n",
        "    y_test_seq = y_seq[train_size + val_size:]\n",
        "\n",
        "    # Prepare data for ensemble models (2D format)\n",
        "    X_train_2d = X_train_seq.reshape(X_train_seq.shape[0], -1)\n",
        "    X_val_2d = X_val_seq.reshape(X_val_seq.shape[0], -1)\n",
        "    X_test_2d = X_test_seq.reshape(X_test_seq.shape[0], -1)\n",
        "\n",
        "    print(f\"Training set shape: {X_train_seq.shape}\")\n",
        "    print(f\"Validation set shape: {X_val_seq.shape}\")\n",
        "    print(f\"Test set shape: {X_test_seq.shape}\")\n",
        "\n",
        "    # Train deep learning models\n",
        "    forecaster.train_deep_models(X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
        "                                seq_length, len(feature_cols))\n",
        "\n",
        "    # Train ensemble models\n",
        "    forecaster.train_ensemble_models(X_train_2d, y_train_seq)\n",
        "\n",
        "    # Evaluate deep learning models\n",
        "    deep_results = forecaster.predict_and_evaluate(X_test_seq, y_test_seq, scaler, 'deep')\n",
        "\n",
        "    # Evaluate ensemble models\n",
        "    ensemble_results = forecaster.predict_and_evaluate(X_test_seq, y_test_seq, scaler, 'ensemble')\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = {**deep_results, **ensemble_results}\n",
        "    forecaster.results = all_results\n",
        "\n",
        "    # Create ensemble prediction\n",
        "    ensemble_result = forecaster.create_ensemble_prediction(all_results)\n",
        "\n",
        "    # Save all models and results\n",
        "    metadata_path = forecaster.save_all_models(scaler, feature_cols, ensemble_result)\n",
        "\n",
        "    # Plot results\n",
        "    forecaster.plot_results(all_results, ensemble_result)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    sorted_results = sorted(all_results.items(), key=lambda x: x[1]['MAE'])\n",
        "\n",
        "    for name, result in sorted_results:\n",
        "        print(f\"{name:15s} - MAE: {result['MAE']:8.4f}, RMSE: {result['RMSE']:8.4f}, \"\n",
        "              f\"MAPE: {result['MAPE']:6.2f}%, R²: {result['R2']:6.4f}\")\n",
        "\n",
        "    print(f\"{'ENSEMBLE':15s} - MAE: {ensemble_result['MAE']:8.4f}, RMSE: {ensemble_result['RMSE']:8.4f}, \"\n",
        "          f\"MAPE: {ensemble_result['MAPE']:6.2f}%, R²: {ensemble_result['R2']:6.4f}\")\n",
        "\n",
        "    print(f\"\\nAll models saved! Metadata file: {metadata_path}\")\n",
        "\n",
        "    return forecaster, all_results, ensemble_result\n",
        "\n",
        "# Function to load and use saved models\n",
        "def load_and_predict():\n",
        "    \"\"\"Function to load saved models and make predictions\"\"\"\n",
        "    forecaster = SolarPowerForecaster()\n",
        "\n",
        "    # List available saved models\n",
        "    sessions = forecaster.list_saved_models()\n",
        "\n",
        "    if not sessions:\n",
        "        print(\"No saved models found!\")\n",
        "        return None\n",
        "\n",
        "    # Load the latest models\n",
        "    print(\"Loading latest saved models...\")\n",
        "    scaler, feature_cols = forecaster.load_all_models()\n",
        "\n",
        "    print(\"Models loaded successfully!\")\n",
        "    print(f\"Available models: {list(forecaster.models.keys())}\")\n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "\n",
        "    return forecaster, scaler, feature_cols\n",
        "\n",
        "# Function to make predictions on new data\n",
        "def predict_new_data(forecaster, scaler, feature_cols, new_data_path, seq_length=48):\n",
        "    \"\"\"Make predictions on new data using loaded models\"\"\"\n",
        "    # Load new data (same preprocessing as training)\n",
        "    # This is a placeholder - you'd implement the same preprocessing pipeline\n",
        "    print(f\"This function would load new data from {new_data_path}\")\n",
        "    print(\"Apply the same preprocessing and feature engineering...\")\n",
        "    print(\"Then make predictions using the loaded models\")\n",
        "\n",
        "    # Example of how to use loaded models:\n",
        "    # predictions = forecaster.predict_with_loaded_models(X_new, scaler)\n",
        "    # ensemble_pred, metrics = forecaster.create_ensemble_prediction_from_loaded(predictions, y_actual)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose whether to train new models or load existing ones\n",
        "    train_new = True  # Set to False to load existing models\n",
        "\n",
        "    if train_new:\n",
        "        print(\"Training new models...\")\n",
        "        forecaster, results, ensemble_result = main()\n",
        "    else:\n",
        "        print(\"Loading existing models...\")\n",
        "        forecaster, scaler, feature_cols = load_and_predict()\n",
        "\n",
        "        # Example: Make predictions on test data\n",
        "        # You would implement the data loading and prediction here\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
