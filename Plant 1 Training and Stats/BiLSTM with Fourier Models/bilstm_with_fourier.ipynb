{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sin, cos, pi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1) Load & preprocess data\n",
    "plant_df = pd.read_csv('Plant1_filtered.csv')\n",
    "weather_df = pd.read_csv('Plant1_Weather_filtered.csv')\n",
    "\n",
    "plant_df['DATE_TIME'] = pd.to_datetime(plant_df['DATE_TIME'])\n",
    "weather_df['DATE_TIME'] = pd.to_datetime(weather_df['DATE_TIME'])\n",
    "\n",
    "# Aggregate plant data - average AC_POWER for each timestamp\n",
    "plant_agg = plant_df.groupby('DATE_TIME').agg({\n",
    "    'AC_POWER': 'mean',\n",
    "    'DC_POWER': 'mean',  # Include if available\n",
    "}).reset_index()\n",
    "\n",
    "# 2) Merge and prepare dataframe\n",
    "df = pd.merge(weather_df, plant_agg, on='DATE_TIME', how='inner')\n",
    "df.set_index('DATE_TIME', inplace=True)\n",
    "df = df.sort_index()  # Ensure time-ordered data\n",
    "\n",
    "# Check for and handle missing values\n",
    "print(f\"Missing values before handling: {df.isnull().sum().sum()}\")\n",
    "df = df.dropna()\n",
    "print(f\"Missing values after handling: {df.isnull().sum().sum()}\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# 3) Advanced feature engineering\n",
    "# Time-based features\n",
    "df['hour'] = df.index.hour + df.index.minute/60.0\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "df['is_weekend'] = df.index.dayofweek >= 5\n",
    "\n",
    "# Multi-frequency Fourier features - capture daily and yearly patterns\n",
    "for period, name in [(24, 'hour'), (168, 'week'), (8760, 'year')]:\n",
    "    for harm in [1, 2, 3]:  # Multiple harmonics for better approximation\n",
    "        df[f'sin_{name}_{harm}'] = np.sin(2*pi*harm*df['hour']/period)\n",
    "        df[f'cos_{name}_{harm}'] = np.cos(2*pi*harm*df['hour']/period)\n",
    "\n",
    "# Basic rolling statistics on key weather variables\n",
    "for col in ['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']:\n",
    "    if col in df.columns:\n",
    "        # Rolling mean - smooths out short-term fluctuations\n",
    "        df[f'{col}_rolling_3h'] = df[col].rolling(window=12).mean()  # 3-hour window (12 * 15min)\n",
    "        # Rolling standard deviation - captures variability/instability\n",
    "        df[f'{col}_rolling_var'] = df[col].rolling(window=12).std()  # Variability\n",
    "\n",
    "# Add lag features for key variables\n",
    "target_col = 'AC_POWER'\n",
    "lag_features = ['AC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']\n",
    "lag_hours = [1, 2, 3, 6, 12]  # Multiple lag periods\n",
    "\n",
    "for col in lag_features:\n",
    "    if col in df.columns:\n",
    "        for lag in lag_hours:\n",
    "            lag_steps = lag * 4  # Assuming 15-minute data (4 steps per hour)\n",
    "            df[f'{col}_lag_{lag}h'] = df[col].shift(lag_steps)\n",
    "\n",
    "# Drop rows with NaN from lag features\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert boolean to int\n",
    "df['is_weekend'] = df['is_weekend'].astype(int)\n",
    "\n",
    "# Drop non-numeric and redundant columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df.drop(['hour', 'day_of_year', 'day_of_week', 'month'], axis=1, inplace=True)\n",
    "print(f\"Final data shape: {df.shape}\")\n",
    "\n",
    "# 4) Select features & target\n",
    "target_col = 'AC_POWER'\n",
    "feature_cols = [col for col in df.columns if col != target_col]\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features sample: {feature_cols[:5]}...\")\n",
    "\n",
    "# 5) Scale features - StandardScaler often works better than MinMaxScaler for LSTMs\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()  # StandardScaler for target too\n",
    "X_all = scaler_X.fit_transform(df[feature_cols])\n",
    "y_all = scaler_y.fit_transform(df[[target_col]])\n",
    "\n",
    "# 6) Improved sequence generator with overlap\n",
    "def create_sequences(X, y, time_steps, horizon, stride=1):\n",
    "    \"\"\"Create sequences with optional stride for more training samples\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps - horizon + 1, stride):\n",
    "        Xs.append(X[i:i + time_steps])\n",
    "        ys.append(y[i + time_steps:i + time_steps + horizon].flatten())\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# 7) Enhanced model architecture\n",
    "def build_enhanced_model(input_shape, horizon):\n",
    "    model = Sequential([\n",
    "        # First BiLSTM layer with more units\n",
    "        Bidirectional(LSTM(256, activation='tanh', return_sequences=True), \n",
    "                     input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),  # Lower dropout\n",
    "        \n",
    "        # Second BiLSTM layer \n",
    "        Bidirectional(LSTM(128, activation='tanh', return_sequences=True)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Third BiLSTM layer\n",
    "        Bidirectional(LSTM(64, activation='tanh')),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(horizon, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Use Adam optimizer with custom learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# 8) Training parameters\n",
    "time_steps = 96  # 24 hours of data (assuming 15-min intervals: 4 * 24 = 96)\n",
    "horizons = [4, 20, 96, 288]  # 1h, 5h, 24h, 72h (4 steps per hour)\n",
    "test_frac = 0.2\n",
    "stride = 2  # Create more training samples with overlapping\n",
    "results = {}\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"TRAINING MODEL FOR HORIZON = {h} STEPS ({h/4} HOURS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create sequences with stride for more training data\n",
    "    X, y = create_sequences(X_all, y_all, time_steps, h, stride=stride)\n",
    "    print(f\"Total sequences: {len(X)}\")\n",
    "    \n",
    "    # Train-test split (time-ordered)\n",
    "    split = int(len(X)*(1-test_frac))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    print(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_enhanced_model((time_steps, X.shape[2]), h)\n",
    "    \n",
    "    # Callbacks for better training\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # More patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,  # More aggressive LR reduction\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit model with more epochs and smaller batch size\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.15,  # Larger validation set\n",
    "        epochs=150,  # More epochs, early stopping will prevent overfitting\n",
    "        batch_size=64,  # Larger batch size for more stable gradients\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # 9) Prediction and evaluation\n",
    "    # Helper function to inverse transform predictions\n",
    "    def inverse_transform(y_scaled):\n",
    "        flat = y_scaled.reshape(-1, 1)\n",
    "        inverted = scaler_y.inverse_transform(flat)\n",
    "        return inverted.reshape(y_scaled.shape)\n",
    "\n",
    "    # Generate predictions\n",
    "    train_pred = model.predict(X_train, verbose=0)\n",
    "    test_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_train_inv = inverse_transform(y_train)\n",
    "    train_pred_inv = inverse_transform(train_pred)\n",
    "    y_test_inv = inverse_transform(y_test)\n",
    "    test_pred_inv = inverse_transform(test_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # For first step prediction\n",
    "    train_mae_first = mean_absolute_error(y_train_inv[:, 0], train_pred_inv[:, 0])\n",
    "    train_rmse_first = np.sqrt(mean_squared_error(y_train_inv[:, 0], train_pred_inv[:, 0]))\n",
    "    train_r2_first = r2_score(y_train_inv[:, 0], train_pred_inv[:, 0])\n",
    "    \n",
    "    test_mae_first = mean_absolute_error(y_test_inv[:, 0], test_pred_inv[:, 0])\n",
    "    test_rmse_first = np.sqrt(mean_squared_error(y_test_inv[:, 0], test_pred_inv[:, 0]))\n",
    "    test_r2_first = r2_score(y_test_inv[:, 0], test_pred_inv[:, 0])\n",
    "    \n",
    "    # For all steps in the horizon\n",
    "    train_mae_all = mean_absolute_error(y_train_inv.flatten(), train_pred_inv.flatten())\n",
    "    train_rmse_all = np.sqrt(mean_squared_error(y_train_inv.flatten(), train_pred_inv.flatten()))\n",
    "    train_r2_all = r2_score(y_train_inv.flatten(), train_pred_inv.flatten())\n",
    "    \n",
    "    test_mae_all = mean_absolute_error(y_test_inv.flatten(), test_pred_inv.flatten())\n",
    "    test_rmse_all = np.sqrt(mean_squared_error(y_test_inv.flatten(), test_pred_inv.flatten()))\n",
    "    test_r2_all = r2_score(y_test_inv.flatten(), test_pred_inv.flatten())\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMETRICS FOR HORIZON = {h} STEPS ({h/4} HOURS):\")\n",
    "    print(f\"FIRST STEP PREDICTIONS:\")\n",
    "    print(f\"  TRAIN - MAE: {train_mae_first:.2f}, RMSE: {train_rmse_first:.2f}, R²: {train_r2_first:.4f}\")\n",
    "    print(f\"  TEST  - MAE: {test_mae_first:.2f}, RMSE: {test_rmse_first:.2f}, R²: {test_r2_first:.4f}\")\n",
    "    print(f\"ALL STEPS PREDICTIONS:\")\n",
    "    print(f\"  TRAIN - MAE: {train_mae_all:.2f}, RMSE: {train_rmse_all:.2f}, R²: {train_r2_all:.4f}\")\n",
    "    print(f\"  TEST  - MAE: {test_mae_all:.2f}, RMSE: {test_rmse_all:.2f}, R²: {test_r2_all:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[h] = {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'y_test': y_test_inv,\n",
    "        'y_pred': test_pred_inv,\n",
    "        'metrics_first': {\n",
    "            'train_mae': train_mae_first,\n",
    "            'train_rmse': train_rmse_first,\n",
    "            'train_r2': train_r2_first,\n",
    "            'test_mae': test_mae_first,\n",
    "            'test_rmse': test_rmse_first,\n",
    "            'test_r2': test_r2_first\n",
    "        },\n",
    "        'metrics_all': {\n",
    "            'train_mae': train_mae_all,\n",
    "            'train_rmse': train_rmse_all,\n",
    "            'train_r2': train_r2_all,\n",
    "            'test_mae': test_mae_all,\n",
    "            'test_rmse': test_rmse_all,\n",
    "            'test_r2': test_r2_all\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 10) Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curves - Horizon {h/4}h')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 11) Plot actual vs predicted for test set\n",
    "    plt.subplot(1, 2, 2)\n",
    "    n_samples = min(200, len(y_test_inv))\n",
    "    \n",
    "    # First step predictions\n",
    "    plt.plot(y_test_inv[:n_samples, 0], 'b-', label='Actual', alpha=0.7)\n",
    "    plt.plot(test_pred_inv[:n_samples, 0], 'r-', label='Predicted', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted AC_POWER - Horizon {h/4}h')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('AC_POWER')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 12) Plot multistep predictions for a few examples\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    # Plot full horizon predictions for a single example\n",
    "    example_idx = 50\n",
    "    steps = np.arange(h)\n",
    "    plt.plot(steps, y_test_inv[example_idx], 'b-', label='Actual', linewidth=2)\n",
    "    plt.plot(steps, test_pred_inv[example_idx], 'r-', label='Predicted', linewidth=2)\n",
    "    plt.title(f'Multi-step Prediction Example - Horizon {h/4}h')\n",
    "    plt.xlabel('Steps Ahead')\n",
    "    plt.ylabel('AC_POWER')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot several first-step predictions\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i in range(min(5, n_samples//40)):\n",
    "        start_idx = i * 40\n",
    "        end_idx = start_idx + 40\n",
    "        plt.plot(y_test_inv[start_idx:end_idx, 0], 'b-', alpha=0.5)\n",
    "        plt.plot(test_pred_inv[start_idx:end_idx, 0], 'r-', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'First-step Predictions - Multiple Segments')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('AC_POWER')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 13) Summary of results\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF RESULTS FOR ALL HORIZONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for h, result in results.items():\n",
    "    metrics_first = result['metrics_first']\n",
    "    metrics_all = result['metrics_all']\n",
    "    \n",
    "    print(f\"\\nHorizon = {h} steps ({h/4} hours):\")\n",
    "    print(f\"FIRST STEP:\")\n",
    "    print(f\"  TEST - MAE: {metrics_first['test_mae']:.2f}, RMSE: {metrics_first['test_rmse']:.2f}, R²: {metrics_first['test_r2']:.4f}\")\n",
    "    print(f\"ALL STEPS:\")\n",
    "    print(f\"  TEST - MAE: {metrics_all['test_mae']:.2f}, RMSE: {metrics_all['test_rmse']:.2f}, R²: {metrics_all['test_r2']:.4f}\")\n",
    "\n",
    "print(\"\\nModel architecture summary:\")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
